{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "ugCXSI3jw-RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "3DNuu70CyMQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class and Metrics"
      ],
      "metadata": {
        "id": "oBksWYpHpJo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FitmentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ViXu8eqoyRv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    #print(classification_report(labels, preds)) # newly added\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "vNyJgNX3yT_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess Data"
      ],
      "metadata": {
        "id": "yFNpQ_9w9Blp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def prepare_data(fold_idx):\n",
        "    # Step 1: Load the predictions file and select the relevant columns\n",
        "    preds = pd.read_csv(f'bc_data/predictions_fold_{fold_idx}.csv')  # INDEX, FTMNT_YEAR, FTMNT_MAKE, FTMNT_MODEL, UN_NORMALIZED\n",
        "\n",
        "    # Select the required columns from the predictions file\n",
        "    new_preds = preds[['INDEX', 'FTMNT_YEAR', 'FTMNT_MAKE', 'FTMNT_MODEL', 'UN_NORMALIZED']].copy()\n",
        "\n",
        "    # Step 2: Add a new column for LABEL and initialize with 0\n",
        "    new_preds['LABEL'] = 0  # Initialize with all zeros\n",
        "\n",
        "    # Step 3: Load the targets file\n",
        "    targets = pd.read_csv(f'bc_data/targets_fold_{fold_idx}.csv')  # INDEX, FTMNT_YEAR, FTMNT_MAKE, FTMNT_MODEL, UN_NORMALIZED\n",
        "\n",
        "    # Step 4: Compare rows in new_preds with the targets\n",
        "    for idx, row in new_preds.iterrows():\n",
        "        # Check if the combination of columns matches any row in the targets\n",
        "        match = targets[\n",
        "            (targets['INDEX'] == row['INDEX']) &\n",
        "            (targets['FTMNT_YEAR'] == row['FTMNT_YEAR']) &\n",
        "            (targets['FTMNT_MAKE'] == row['FTMNT_MAKE']) &\n",
        "            (targets['FTMNT_MODEL'] == row['FTMNT_MODEL']) &\n",
        "            (targets['UN_NORMALIZED'] == row['UN_NORMALIZED'])\n",
        "        ]\n",
        "\n",
        "        # If there's a match, set LABEL to 1 (true positive)\n",
        "        if not match.empty:\n",
        "            new_preds.at[idx, 'LABEL'] = 1  # Update the LABEL column to 1 for true positive\n",
        "\n",
        "    # Step 5: Save the updated predictions to a new CSV file\n",
        "    #new_preds.to_csv('new_prediction.csv', index=False)\n",
        "    # return new_preds\n",
        "    # print(f\"new_prediction.csv file created for fold {fold_idx}\")\n",
        "\n",
        "\n",
        "        # Step 2: Load the data_fold_i.csv file\n",
        "    data_fold = pd.read_csv(f'bc_data/data_fold_{fold_idx}.csv')  # Contains index, prompt, response, cat\n",
        "\n",
        "    # Step 3: Initialize an empty DataFrame for the final output\n",
        "    final_data = pd.DataFrame(columns=['INDEX', 'prompt', 'response', 'cat', 'FTMNT_YEAR', 'FTMNT_MAKE', 'FTMNT_MODEL', 'UN_NORMALIZED', 'LABEL'])\n",
        "\n",
        "    # Step 4: Merge the two datasets based on the INDEX\n",
        "    for _, row in new_preds.iterrows():\n",
        "        # Find the corresponding row in data_fold_i.csv where index == INDEX\n",
        "        match = data_fold[data_fold['index'] == row['INDEX']]\n",
        "\n",
        "        # If a match is found, create a new row in the final data\n",
        "        if not match.empty:\n",
        "            # Extract prompt, response, and cat from data_fold\n",
        "            final_row = {\n",
        "                'INDEX': row['INDEX'],\n",
        "                'prompt': match.iloc[0]['prompt'],\n",
        "                'response': match.iloc[0]['response'],\n",
        "                'cat': match.iloc[0]['cat'],\n",
        "                'FTMNT_YEAR': row['FTMNT_YEAR'],\n",
        "                'FTMNT_MAKE': row['FTMNT_MAKE'],\n",
        "                'FTMNT_MODEL': row['FTMNT_MODEL'],\n",
        "                'UN_NORMALIZED': row['UN_NORMALIZED'],\n",
        "                'LABEL': row['LABEL']\n",
        "            }\n",
        "            # Append the new row to the final_data DataFrame\n",
        "            final_data = final_data.append(final_row, ignore_index=True)\n",
        "\n",
        "    # Step 5: Save the final_data DataFrame to final_data_fold_i.csv\n",
        "    final_data.to_csv(f'data/final_data_fold_{fold_idx}.csv', index=False)\n",
        "    print(f\"final_data_fold_{fold_idx}.csv file created successfully.\")\n",
        "    #final_data.to_csv('final_data.csv', index=False)\n",
        "\n",
        "    return final_data\n"
      ],
      "metadata": {
        "id": "t_tzgKCN9C3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Prepare data for fold 0\n",
        "import csv\n",
        "\n",
        "final_processed_Data = prepare_data(4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLww2KGa9eSs",
        "outputId": "aa7f6c90-c19d-4c3e-ca35-247f9821655c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_data_fold_4.csv file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = \"data\"\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Specify the full file path (directory + file name)\n",
        "file_path = os.path.join(directory, 'output_file.csv')\n",
        "\n",
        "# Save the file_P DataFrame to the specified directory\n",
        "file_path.to_csv(final_processed_Data, index=False)\n",
        "\n",
        "print(f\"CSV file saved successfully to {file_path}\")"
      ],
      "metadata": {
        "id": "lGv5U3E9j1oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(f'data/final_data_fold_4.csv')\n",
        "true_positives = data[data['LABEL'] == 0]\n",
        "#false_positives = new_preds[file_path['LABEL'] == 0]\n",
        "print(true_positives)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS36BFTC-_Py",
        "outputId": "a06da7f2-0d6f-4ceb-a498-9c0b9cb8087f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      INDEX  ... LABEL\n",
            "38       11  ...     0\n",
            "39       11  ...     0\n",
            "41       11  ...     0\n",
            "84       20  ...     0\n",
            "122      26  ...     0\n",
            "...     ...  ...   ...\n",
            "6975    993  ...     0\n",
            "6976    993  ...     0\n",
            "6977    993  ...     0\n",
            "6978    993  ...     0\n",
            "7027    999  ...     0\n",
            "\n",
            "[1013 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Train and Test Dataset"
      ],
      "metadata": {
        "id": "Qpfe72CMkReB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_train_test_data(fold_idx,tokenizer, max_len):\n",
        "\n",
        "        # Load the final_data_fold_i.csv file\n",
        "\n",
        "\n",
        "    # Separate the features and the label\n",
        "    # You can use 'prompt' or 'response' depending on the task, let's assume 'prompt'\n",
        "    # texts = data['response'].tolist()\n",
        "    # labels = data['LABEL'].tolist()\n",
        "\n",
        "    train_texts, train_labels = [], []\n",
        "\n",
        "    for i in range(5):\n",
        "        if i==fold_idx:\n",
        "            continue\n",
        "        data = pd.read_csv(f'data/final_data_fold_{i}.csv')\n",
        "\n",
        "        for _, row in data.iterrows():\n",
        "            train_texts.append(row['response'])\n",
        "            train_labels.append(row['LABEL'])\n",
        "\n",
        "\n",
        "\n",
        "    test_texts, test_labels = [], []\n",
        "    data = pd.read_csv(f'data/final_data_fold_{fold_idx}.csv')\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        test_texts.append(row['response'])\n",
        "        test_labels.append(row['LABEL'])\n",
        "\n",
        "      # Tokenize the data\n",
        "    train_dataset = FitmentDataset(train_texts, train_labels, tokenizer, max_len)\n",
        "    test_dataset = FitmentDataset(test_texts, test_labels, tokenizer, max_len)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "0qU3ahLSkUA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and Evaluate"
      ],
      "metadata": {
        "id": "AQ-1UaNkj4tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(fold_idx, tokenizer, model, max_len, batch_size, epochs):\n",
        "    train_dataset, test_dataset = prepare_train_test_data(fold_idx, tokenizer, max_len)\n",
        "\n",
        "    # Set up the Trainer\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_fold_{fold_idx}',\n",
        "        #evaluation_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1',\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=10\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model on test fold\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    print(f\"Fold {fold_idx} results:\", eval_results)\n",
        "    return eval_results\n"
      ],
      "metadata": {
        "id": "wSZOMaz8yYRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def main():\n",
        "# Load the tokenizer and model\n",
        "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "# model = model.to(device)\n",
        "\n",
        "from transformers import DebertaTokenizerFast, DebertaForSequenceClassification\n",
        "\n",
        "# Load the tokenizer and model for DeBERTa\n",
        "tokenizer = DebertaTokenizerFast.from_pretrained('microsoft/deberta-base')\n",
        "model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=2)\n",
        "\n",
        "# Move the model to the device (GPU/CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "max_len = 128\n",
        "batch_size = 16\n",
        "epochs = 3\n",
        "\n",
        "# Train and evaluate for each fold\n",
        "all_results = []\n",
        "for fold_idx in range(5): # Place 5 for all fold results\n",
        "    print(f\"Training on fold {fold_idx}...\")\n",
        "    results = train_and_evaluate(fold_idx, tokenizer, model, max_len, batch_size, epochs)\n",
        "    print(f\"Fold {fold_idx} results:\", results)\n",
        "    all_results.append(results)\n",
        "\n",
        "print(\"All fold results:\", all_results)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "gUYqF3mWyeZx",
        "outputId": "6db53151-ed57-4ecf-f180-5d9446bcfcea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on fold 0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1395' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1395/1395 43:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.348691</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.939742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.439500</td>\n",
              "      <td>0.364920</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.939742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.399200</td>\n",
              "      <td>0.375950</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.939742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.433000</td>\n",
              "      <td>0.339222</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.939742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.360600</td>\n",
              "      <td>0.353628</td>\n",
              "      <td>0.883876</td>\n",
              "      <td>0.891387</td>\n",
              "      <td>0.989558</td>\n",
              "      <td>0.937911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.419700</td>\n",
              "      <td>0.349529</td>\n",
              "      <td>0.876790</td>\n",
              "      <td>0.886367</td>\n",
              "      <td>0.987600</td>\n",
              "      <td>0.934249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.412300</td>\n",
              "      <td>0.331394</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>0.886334</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.939742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.338900</td>\n",
              "      <td>0.331507</td>\n",
              "      <td>0.883731</td>\n",
              "      <td>0.886710</td>\n",
              "      <td>0.996084</td>\n",
              "      <td>0.938220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.437600</td>\n",
              "      <td>0.331494</td>\n",
              "      <td>0.883442</td>\n",
              "      <td>0.895880</td>\n",
              "      <td>0.982705</td>\n",
              "      <td>0.937286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.374300</td>\n",
              "      <td>0.341898</td>\n",
              "      <td>0.881562</td>\n",
              "      <td>0.893100</td>\n",
              "      <td>0.984174</td>\n",
              "      <td>0.936428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.383800</td>\n",
              "      <td>0.336680</td>\n",
              "      <td>0.884743</td>\n",
              "      <td>0.887725</td>\n",
              "      <td>0.995921</td>\n",
              "      <td>0.938716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.434600</td>\n",
              "      <td>0.337221</td>\n",
              "      <td>0.883876</td>\n",
              "      <td>0.886727</td>\n",
              "      <td>0.996247</td>\n",
              "      <td>0.938302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.432600</td>\n",
              "      <td>0.363833</td>\n",
              "      <td>0.874910</td>\n",
              "      <td>0.895314</td>\n",
              "      <td>0.972589</td>\n",
              "      <td>0.932353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.415600</td>\n",
              "      <td>0.326197</td>\n",
              "      <td>0.888648</td>\n",
              "      <td>0.891568</td>\n",
              "      <td>0.995432</td>\n",
              "      <td>0.940641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.413800</td>\n",
              "      <td>0.337630</td>\n",
              "      <td>0.887925</td>\n",
              "      <td>0.890575</td>\n",
              "      <td>0.995921</td>\n",
              "      <td>0.940307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.404100</td>\n",
              "      <td>0.355987</td>\n",
              "      <td>0.854375</td>\n",
              "      <td>0.893879</td>\n",
              "      <td>0.948279</td>\n",
              "      <td>0.920276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.421100</td>\n",
              "      <td>0.335067</td>\n",
              "      <td>0.887057</td>\n",
              "      <td>0.893235</td>\n",
              "      <td>0.991026</td>\n",
              "      <td>0.939593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.403400</td>\n",
              "      <td>0.348408</td>\n",
              "      <td>0.886768</td>\n",
              "      <td>0.890447</td>\n",
              "      <td>0.994616</td>\n",
              "      <td>0.939653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.414900</td>\n",
              "      <td>0.374014</td>\n",
              "      <td>0.846855</td>\n",
              "      <td>0.892051</td>\n",
              "      <td>0.941100</td>\n",
              "      <td>0.915919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.387700</td>\n",
              "      <td>0.342348</td>\n",
              "      <td>0.884888</td>\n",
              "      <td>0.886169</td>\n",
              "      <td>0.998368</td>\n",
              "      <td>0.938929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.450600</td>\n",
              "      <td>0.364002</td>\n",
              "      <td>0.847867</td>\n",
              "      <td>0.893749</td>\n",
              "      <td>0.940121</td>\n",
              "      <td>0.916349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.395800</td>\n",
              "      <td>0.333815</td>\n",
              "      <td>0.885466</td>\n",
              "      <td>0.889619</td>\n",
              "      <td>0.994126</td>\n",
              "      <td>0.938974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.368200</td>\n",
              "      <td>0.371795</td>\n",
              "      <td>0.820824</td>\n",
              "      <td>0.898988</td>\n",
              "      <td>0.898842</td>\n",
              "      <td>0.898915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.369300</td>\n",
              "      <td>0.372748</td>\n",
              "      <td>0.825307</td>\n",
              "      <td>0.894627</td>\n",
              "      <td>0.910100</td>\n",
              "      <td>0.902297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.409800</td>\n",
              "      <td>0.332192</td>\n",
              "      <td>0.884454</td>\n",
              "      <td>0.892605</td>\n",
              "      <td>0.988579</td>\n",
              "      <td>0.938144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.341400</td>\n",
              "      <td>0.331492</td>\n",
              "      <td>0.887491</td>\n",
              "      <td>0.892245</td>\n",
              "      <td>0.992984</td>\n",
              "      <td>0.939923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.398600</td>\n",
              "      <td>0.346622</td>\n",
              "      <td>0.878380</td>\n",
              "      <td>0.902435</td>\n",
              "      <td>0.967368</td>\n",
              "      <td>0.933774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.384000</td>\n",
              "      <td>0.332380</td>\n",
              "      <td>0.878670</td>\n",
              "      <td>0.895130</td>\n",
              "      <td>0.977647</td>\n",
              "      <td>0.934571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.324200</td>\n",
              "      <td>0.356512</td>\n",
              "      <td>0.868547</td>\n",
              "      <td>0.896295</td>\n",
              "      <td>0.963126</td>\n",
              "      <td>0.928510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.358800</td>\n",
              "      <td>0.355075</td>\n",
              "      <td>0.874476</td>\n",
              "      <td>0.896458</td>\n",
              "      <td>0.970468</td>\n",
              "      <td>0.931996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.378100</td>\n",
              "      <td>0.350850</td>\n",
              "      <td>0.883876</td>\n",
              "      <td>0.897938</td>\n",
              "      <td>0.980421</td>\n",
              "      <td>0.937368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.400200</td>\n",
              "      <td>0.343417</td>\n",
              "      <td>0.880260</td>\n",
              "      <td>0.899593</td>\n",
              "      <td>0.973568</td>\n",
              "      <td>0.935120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.378500</td>\n",
              "      <td>0.332215</td>\n",
              "      <td>0.880550</td>\n",
              "      <td>0.893807</td>\n",
              "      <td>0.981889</td>\n",
              "      <td>0.935780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.413900</td>\n",
              "      <td>0.372690</td>\n",
              "      <td>0.843673</td>\n",
              "      <td>0.896855</td>\n",
              "      <td>0.930658</td>\n",
              "      <td>0.913444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.356800</td>\n",
              "      <td>0.355687</td>\n",
              "      <td>0.877368</td>\n",
              "      <td>0.894517</td>\n",
              "      <td>0.976831</td>\n",
              "      <td>0.933864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.403300</td>\n",
              "      <td>0.339412</td>\n",
              "      <td>0.883586</td>\n",
              "      <td>0.892047</td>\n",
              "      <td>0.988253</td>\n",
              "      <td>0.937689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.354000</td>\n",
              "      <td>0.343401</td>\n",
              "      <td>0.884599</td>\n",
              "      <td>0.896829</td>\n",
              "      <td>0.982868</td>\n",
              "      <td>0.937879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.366400</td>\n",
              "      <td>0.346762</td>\n",
              "      <td>0.878091</td>\n",
              "      <td>0.898883</td>\n",
              "      <td>0.971774</td>\n",
              "      <td>0.933908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.320600</td>\n",
              "      <td>0.345340</td>\n",
              "      <td>0.879971</td>\n",
              "      <td>0.893861</td>\n",
              "      <td>0.981074</td>\n",
              "      <td>0.935439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.343100</td>\n",
              "      <td>0.356208</td>\n",
              "      <td>0.871584</td>\n",
              "      <td>0.898192</td>\n",
              "      <td>0.964431</td>\n",
              "      <td>0.930134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.366300</td>\n",
              "      <td>0.340513</td>\n",
              "      <td>0.877802</td>\n",
              "      <td>0.898973</td>\n",
              "      <td>0.971284</td>\n",
              "      <td>0.933731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.370100</td>\n",
              "      <td>0.335639</td>\n",
              "      <td>0.880983</td>\n",
              "      <td>0.896562</td>\n",
              "      <td>0.978626</td>\n",
              "      <td>0.935798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.416500</td>\n",
              "      <td>0.381240</td>\n",
              "      <td>0.857267</td>\n",
              "      <td>0.900343</td>\n",
              "      <td>0.943384</td>\n",
              "      <td>0.921361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.366200</td>\n",
              "      <td>0.346640</td>\n",
              "      <td>0.882430</td>\n",
              "      <td>0.897428</td>\n",
              "      <td>0.979279</td>\n",
              "      <td>0.936569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.356200</td>\n",
              "      <td>0.343422</td>\n",
              "      <td>0.879682</td>\n",
              "      <td>0.897255</td>\n",
              "      <td>0.976016</td>\n",
              "      <td>0.934980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.388600</td>\n",
              "      <td>0.370095</td>\n",
              "      <td>0.852928</td>\n",
              "      <td>0.897759</td>\n",
              "      <td>0.941263</td>\n",
              "      <td>0.918996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.323700</td>\n",
              "      <td>0.365427</td>\n",
              "      <td>0.874910</td>\n",
              "      <td>0.895552</td>\n",
              "      <td>0.972263</td>\n",
              "      <td>0.932332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.324000</td>\n",
              "      <td>0.369613</td>\n",
              "      <td>0.872451</td>\n",
              "      <td>0.898648</td>\n",
              "      <td>0.964921</td>\n",
              "      <td>0.930606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.340600</td>\n",
              "      <td>0.353642</td>\n",
              "      <td>0.874476</td>\n",
              "      <td>0.896578</td>\n",
              "      <td>0.970305</td>\n",
              "      <td>0.931986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.347900</td>\n",
              "      <td>0.344858</td>\n",
              "      <td>0.878525</td>\n",
              "      <td>0.893585</td>\n",
              "      <td>0.979605</td>\n",
              "      <td>0.934620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.365500</td>\n",
              "      <td>0.378474</td>\n",
              "      <td>0.851193</td>\n",
              "      <td>0.900566</td>\n",
              "      <td>0.935389</td>\n",
              "      <td>0.917647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.375400</td>\n",
              "      <td>0.356685</td>\n",
              "      <td>0.871294</td>\n",
              "      <td>0.897074</td>\n",
              "      <td>0.965574</td>\n",
              "      <td>0.930064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.351000</td>\n",
              "      <td>0.371952</td>\n",
              "      <td>0.872596</td>\n",
              "      <td>0.901961</td>\n",
              "      <td>0.960679</td>\n",
              "      <td>0.930394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.358600</td>\n",
              "      <td>0.339799</td>\n",
              "      <td>0.880405</td>\n",
              "      <td>0.896619</td>\n",
              "      <td>0.977810</td>\n",
              "      <td>0.935456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.332300</td>\n",
              "      <td>0.343163</td>\n",
              "      <td>0.875343</td>\n",
              "      <td>0.894768</td>\n",
              "      <td>0.973895</td>\n",
              "      <td>0.932656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.358100</td>\n",
              "      <td>0.356387</td>\n",
              "      <td>0.874620</td>\n",
              "      <td>0.900091</td>\n",
              "      <td>0.965737</td>\n",
              "      <td>0.931759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.338000</td>\n",
              "      <td>0.349134</td>\n",
              "      <td>0.883297</td>\n",
              "      <td>0.895982</td>\n",
              "      <td>0.982379</td>\n",
              "      <td>0.937194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.325600</td>\n",
              "      <td>0.355319</td>\n",
              "      <td>0.874476</td>\n",
              "      <td>0.901052</td>\n",
              "      <td>0.964268</td>\n",
              "      <td>0.931589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.342100</td>\n",
              "      <td>0.361952</td>\n",
              "      <td>0.872451</td>\n",
              "      <td>0.898043</td>\n",
              "      <td>0.965737</td>\n",
              "      <td>0.930660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.344000</td>\n",
              "      <td>0.359964</td>\n",
              "      <td>0.881996</td>\n",
              "      <td>0.897145</td>\n",
              "      <td>0.979116</td>\n",
              "      <td>0.936340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.340500</td>\n",
              "      <td>0.348201</td>\n",
              "      <td>0.883876</td>\n",
              "      <td>0.896516</td>\n",
              "      <td>0.982379</td>\n",
              "      <td>0.937485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.309000</td>\n",
              "      <td>0.343105</td>\n",
              "      <td>0.884888</td>\n",
              "      <td>0.894745</td>\n",
              "      <td>0.986132</td>\n",
              "      <td>0.938218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.335300</td>\n",
              "      <td>0.349842</td>\n",
              "      <td>0.885611</td>\n",
              "      <td>0.898358</td>\n",
              "      <td>0.982053</td>\n",
              "      <td>0.938343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.319100</td>\n",
              "      <td>0.347656</td>\n",
              "      <td>0.881851</td>\n",
              "      <td>0.896418</td>\n",
              "      <td>0.979931</td>\n",
              "      <td>0.936316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.340100</td>\n",
              "      <td>0.369859</td>\n",
              "      <td>0.858713</td>\n",
              "      <td>0.899256</td>\n",
              "      <td>0.946647</td>\n",
              "      <td>0.922343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.299000</td>\n",
              "      <td>0.356421</td>\n",
              "      <td>0.883731</td>\n",
              "      <td>0.897447</td>\n",
              "      <td>0.980910</td>\n",
              "      <td>0.937325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.320400</td>\n",
              "      <td>0.367310</td>\n",
              "      <td>0.863196</td>\n",
              "      <td>0.896800</td>\n",
              "      <td>0.955621</td>\n",
              "      <td>0.925276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.305900</td>\n",
              "      <td>0.364771</td>\n",
              "      <td>0.879682</td>\n",
              "      <td>0.898091</td>\n",
              "      <td>0.974874</td>\n",
              "      <td>0.934908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.298400</td>\n",
              "      <td>0.369510</td>\n",
              "      <td>0.878091</td>\n",
              "      <td>0.899124</td>\n",
              "      <td>0.971447</td>\n",
              "      <td>0.933888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.325100</td>\n",
              "      <td>0.374653</td>\n",
              "      <td>0.867968</td>\n",
              "      <td>0.903965</td>\n",
              "      <td>0.952194</td>\n",
              "      <td>0.927453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.312900</td>\n",
              "      <td>0.354069</td>\n",
              "      <td>0.877513</td>\n",
              "      <td>0.899546</td>\n",
              "      <td>0.970142</td>\n",
              "      <td>0.933511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.331300</td>\n",
              "      <td>0.352245</td>\n",
              "      <td>0.873319</td>\n",
              "      <td>0.899589</td>\n",
              "      <td>0.964758</td>\n",
              "      <td>0.931034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.351700</td>\n",
              "      <td>0.358766</td>\n",
              "      <td>0.872740</td>\n",
              "      <td>0.899528</td>\n",
              "      <td>0.964105</td>\n",
              "      <td>0.930698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.309900</td>\n",
              "      <td>0.355750</td>\n",
              "      <td>0.874910</td>\n",
              "      <td>0.898426</td>\n",
              "      <td>0.968347</td>\n",
              "      <td>0.932077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.283400</td>\n",
              "      <td>0.361814</td>\n",
              "      <td>0.881562</td>\n",
              "      <td>0.896032</td>\n",
              "      <td>0.980095</td>\n",
              "      <td>0.936180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.307000</td>\n",
              "      <td>0.365919</td>\n",
              "      <td>0.872162</td>\n",
              "      <td>0.894658</td>\n",
              "      <td>0.969979</td>\n",
              "      <td>0.930797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.317100</td>\n",
              "      <td>0.362314</td>\n",
              "      <td>0.869270</td>\n",
              "      <td>0.895175</td>\n",
              "      <td>0.965574</td>\n",
              "      <td>0.929042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.318400</td>\n",
              "      <td>0.377015</td>\n",
              "      <td>0.849458</td>\n",
              "      <td>0.900504</td>\n",
              "      <td>0.933268</td>\n",
              "      <td>0.916593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.297400</td>\n",
              "      <td>0.367655</td>\n",
              "      <td>0.883008</td>\n",
              "      <td>0.893258</td>\n",
              "      <td>0.985805</td>\n",
              "      <td>0.937253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.354800</td>\n",
              "      <td>0.362432</td>\n",
              "      <td>0.857701</td>\n",
              "      <td>0.900265</td>\n",
              "      <td>0.944037</td>\n",
              "      <td>0.921631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.349700</td>\n",
              "      <td>0.350377</td>\n",
              "      <td>0.874331</td>\n",
              "      <td>0.896562</td>\n",
              "      <td>0.970142</td>\n",
              "      <td>0.931902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.331500</td>\n",
              "      <td>0.348197</td>\n",
              "      <td>0.880839</td>\n",
              "      <td>0.896665</td>\n",
              "      <td>0.978300</td>\n",
              "      <td>0.935705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.300900</td>\n",
              "      <td>0.361686</td>\n",
              "      <td>0.873319</td>\n",
              "      <td>0.897413</td>\n",
              "      <td>0.967695</td>\n",
              "      <td>0.931229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.319400</td>\n",
              "      <td>0.385651</td>\n",
              "      <td>0.884743</td>\n",
              "      <td>0.895549</td>\n",
              "      <td>0.984826</td>\n",
              "      <td>0.938068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.327400</td>\n",
              "      <td>0.379608</td>\n",
              "      <td>0.858713</td>\n",
              "      <td>0.897899</td>\n",
              "      <td>0.948442</td>\n",
              "      <td>0.922479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.307800</td>\n",
              "      <td>0.370367</td>\n",
              "      <td>0.873753</td>\n",
              "      <td>0.894950</td>\n",
              "      <td>0.971610</td>\n",
              "      <td>0.931706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.317300</td>\n",
              "      <td>0.374182</td>\n",
              "      <td>0.868402</td>\n",
              "      <td>0.895919</td>\n",
              "      <td>0.963452</td>\n",
              "      <td>0.928459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.314300</td>\n",
              "      <td>0.364407</td>\n",
              "      <td>0.879537</td>\n",
              "      <td>0.890445</td>\n",
              "      <td>0.985316</td>\n",
              "      <td>0.935481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.351900</td>\n",
              "      <td>0.370196</td>\n",
              "      <td>0.849747</td>\n",
              "      <td>0.895924</td>\n",
              "      <td>0.939631</td>\n",
              "      <td>0.917257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.339300</td>\n",
              "      <td>0.367708</td>\n",
              "      <td>0.872740</td>\n",
              "      <td>0.894128</td>\n",
              "      <td>0.971447</td>\n",
              "      <td>0.931185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.289600</td>\n",
              "      <td>0.363845</td>\n",
              "      <td>0.880694</td>\n",
              "      <td>0.898677</td>\n",
              "      <td>0.975363</td>\n",
              "      <td>0.935451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.332900</td>\n",
              "      <td>0.361958</td>\n",
              "      <td>0.868402</td>\n",
              "      <td>0.899801</td>\n",
              "      <td>0.958231</td>\n",
              "      <td>0.928097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.367529</td>\n",
              "      <td>0.879826</td>\n",
              "      <td>0.892794</td>\n",
              "      <td>0.982379</td>\n",
              "      <td>0.935446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.302600</td>\n",
              "      <td>0.369896</td>\n",
              "      <td>0.879971</td>\n",
              "      <td>0.896929</td>\n",
              "      <td>0.976831</td>\n",
              "      <td>0.935177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.286600</td>\n",
              "      <td>0.376375</td>\n",
              "      <td>0.874910</td>\n",
              "      <td>0.896147</td>\n",
              "      <td>0.971447</td>\n",
              "      <td>0.932279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.321100</td>\n",
              "      <td>0.387055</td>\n",
              "      <td>0.858858</td>\n",
              "      <td>0.898654</td>\n",
              "      <td>0.947626</td>\n",
              "      <td>0.922490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.267700</td>\n",
              "      <td>0.381445</td>\n",
              "      <td>0.879103</td>\n",
              "      <td>0.894941</td>\n",
              "      <td>0.978463</td>\n",
              "      <td>0.934840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.337300</td>\n",
              "      <td>0.373170</td>\n",
              "      <td>0.868547</td>\n",
              "      <td>0.899327</td>\n",
              "      <td>0.959047</td>\n",
              "      <td>0.928227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.315400</td>\n",
              "      <td>0.372011</td>\n",
              "      <td>0.872740</td>\n",
              "      <td>0.898436</td>\n",
              "      <td>0.965574</td>\n",
              "      <td>0.930796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.272700</td>\n",
              "      <td>0.376686</td>\n",
              "      <td>0.882285</td>\n",
              "      <td>0.897413</td>\n",
              "      <td>0.979116</td>\n",
              "      <td>0.936486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.343500</td>\n",
              "      <td>0.378809</td>\n",
              "      <td>0.858568</td>\n",
              "      <td>0.897392</td>\n",
              "      <td>0.948931</td>\n",
              "      <td>0.922443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.263600</td>\n",
              "      <td>0.377365</td>\n",
              "      <td>0.869704</td>\n",
              "      <td>0.898597</td>\n",
              "      <td>0.961495</td>\n",
              "      <td>0.928982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.380066</td>\n",
              "      <td>0.873753</td>\n",
              "      <td>0.898544</td>\n",
              "      <td>0.966716</td>\n",
              "      <td>0.931384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.299200</td>\n",
              "      <td>0.375920</td>\n",
              "      <td>0.865221</td>\n",
              "      <td>0.899462</td>\n",
              "      <td>0.954642</td>\n",
              "      <td>0.926231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.243300</td>\n",
              "      <td>0.378993</td>\n",
              "      <td>0.878959</td>\n",
              "      <td>0.899940</td>\n",
              "      <td>0.971447</td>\n",
              "      <td>0.934327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.287200</td>\n",
              "      <td>0.380454</td>\n",
              "      <td>0.870716</td>\n",
              "      <td>0.899557</td>\n",
              "      <td>0.961495</td>\n",
              "      <td>0.929495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.316200</td>\n",
              "      <td>0.378970</td>\n",
              "      <td>0.872017</td>\n",
              "      <td>0.896912</td>\n",
              "      <td>0.966716</td>\n",
              "      <td>0.930506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.292000</td>\n",
              "      <td>0.378810</td>\n",
              "      <td>0.874476</td>\n",
              "      <td>0.894911</td>\n",
              "      <td>0.972589</td>\n",
              "      <td>0.932134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.296800</td>\n",
              "      <td>0.383114</td>\n",
              "      <td>0.865221</td>\n",
              "      <td>0.897385</td>\n",
              "      <td>0.957416</td>\n",
              "      <td>0.926429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.296400</td>\n",
              "      <td>0.378781</td>\n",
              "      <td>0.879393</td>\n",
              "      <td>0.893446</td>\n",
              "      <td>0.980910</td>\n",
              "      <td>0.935138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.319200</td>\n",
              "      <td>0.373555</td>\n",
              "      <td>0.881706</td>\n",
              "      <td>0.893699</td>\n",
              "      <td>0.983521</td>\n",
              "      <td>0.936461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.256300</td>\n",
              "      <td>0.373120</td>\n",
              "      <td>0.877223</td>\n",
              "      <td>0.893325</td>\n",
              "      <td>0.978300</td>\n",
              "      <td>0.933884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>0.293800</td>\n",
              "      <td>0.375129</td>\n",
              "      <td>0.879537</td>\n",
              "      <td>0.895696</td>\n",
              "      <td>0.977974</td>\n",
              "      <td>0.935028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.300700</td>\n",
              "      <td>0.381732</td>\n",
              "      <td>0.867390</td>\n",
              "      <td>0.897620</td>\n",
              "      <td>0.959863</td>\n",
              "      <td>0.927698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.279200</td>\n",
              "      <td>0.389024</td>\n",
              "      <td>0.869559</td>\n",
              "      <td>0.897732</td>\n",
              "      <td>0.962473</td>\n",
              "      <td>0.928976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.337700</td>\n",
              "      <td>0.390962</td>\n",
              "      <td>0.868402</td>\n",
              "      <td>0.898336</td>\n",
              "      <td>0.960189</td>\n",
              "      <td>0.928233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.280200</td>\n",
              "      <td>0.391048</td>\n",
              "      <td>0.867679</td>\n",
              "      <td>0.897894</td>\n",
              "      <td>0.959863</td>\n",
              "      <td>0.927845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.276700</td>\n",
              "      <td>0.388740</td>\n",
              "      <td>0.876934</td>\n",
              "      <td>0.897440</td>\n",
              "      <td>0.972263</td>\n",
              "      <td>0.933354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.321400</td>\n",
              "      <td>0.391950</td>\n",
              "      <td>0.872307</td>\n",
              "      <td>0.897304</td>\n",
              "      <td>0.966552</td>\n",
              "      <td>0.930642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.329000</td>\n",
              "      <td>0.394042</td>\n",
              "      <td>0.853651</td>\n",
              "      <td>0.896237</td>\n",
              "      <td>0.944200</td>\n",
              "      <td>0.919593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.282100</td>\n",
              "      <td>0.380338</td>\n",
              "      <td>0.878814</td>\n",
              "      <td>0.896211</td>\n",
              "      <td>0.976342</td>\n",
              "      <td>0.934562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.339900</td>\n",
              "      <td>0.378310</td>\n",
              "      <td>0.871439</td>\n",
              "      <td>0.895412</td>\n",
              "      <td>0.968021</td>\n",
              "      <td>0.930302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.306700</td>\n",
              "      <td>0.381612</td>\n",
              "      <td>0.863196</td>\n",
              "      <td>0.894504</td>\n",
              "      <td>0.958721</td>\n",
              "      <td>0.925500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.383559</td>\n",
              "      <td>0.869270</td>\n",
              "      <td>0.894816</td>\n",
              "      <td>0.966063</td>\n",
              "      <td>0.929076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.267900</td>\n",
              "      <td>0.389700</td>\n",
              "      <td>0.862328</td>\n",
              "      <td>0.894288</td>\n",
              "      <td>0.957905</td>\n",
              "      <td>0.925004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>0.275600</td>\n",
              "      <td>0.391584</td>\n",
              "      <td>0.869414</td>\n",
              "      <td>0.896751</td>\n",
              "      <td>0.963616</td>\n",
              "      <td>0.928982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>0.308700</td>\n",
              "      <td>0.389660</td>\n",
              "      <td>0.863630</td>\n",
              "      <td>0.895395</td>\n",
              "      <td>0.958068</td>\n",
              "      <td>0.925672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>0.310500</td>\n",
              "      <td>0.388190</td>\n",
              "      <td>0.857411</td>\n",
              "      <td>0.894947</td>\n",
              "      <td>0.950726</td>\n",
              "      <td>0.921994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>0.289100</td>\n",
              "      <td>0.387616</td>\n",
              "      <td>0.877513</td>\n",
              "      <td>0.895833</td>\n",
              "      <td>0.975200</td>\n",
              "      <td>0.933833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.289700</td>\n",
              "      <td>0.388433</td>\n",
              "      <td>0.857411</td>\n",
              "      <td>0.894947</td>\n",
              "      <td>0.950726</td>\n",
              "      <td>0.921994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>0.308200</td>\n",
              "      <td>0.388207</td>\n",
              "      <td>0.857701</td>\n",
              "      <td>0.894979</td>\n",
              "      <td>0.951052</td>\n",
              "      <td>0.922164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>0.287700</td>\n",
              "      <td>0.387108</td>\n",
              "      <td>0.872885</td>\n",
              "      <td>0.895928</td>\n",
              "      <td>0.969163</td>\n",
              "      <td>0.931107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>0.279300</td>\n",
              "      <td>0.386700</td>\n",
              "      <td>0.875777</td>\n",
              "      <td>0.896121</td>\n",
              "      <td>0.972589</td>\n",
              "      <td>0.932791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>0.336400</td>\n",
              "      <td>0.386115</td>\n",
              "      <td>0.878380</td>\n",
              "      <td>0.895927</td>\n",
              "      <td>0.976179</td>\n",
              "      <td>0.934333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.267700</td>\n",
              "      <td>0.386714</td>\n",
              "      <td>0.877223</td>\n",
              "      <td>0.896277</td>\n",
              "      <td>0.974221</td>\n",
              "      <td>0.933625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>0.274500</td>\n",
              "      <td>0.388163</td>\n",
              "      <td>0.877223</td>\n",
              "      <td>0.896277</td>\n",
              "      <td>0.974221</td>\n",
              "      <td>0.933625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>0.281300</td>\n",
              "      <td>0.388523</td>\n",
              "      <td>0.877223</td>\n",
              "      <td>0.896277</td>\n",
              "      <td>0.974221</td>\n",
              "      <td>0.933625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.297000</td>\n",
              "      <td>0.388752</td>\n",
              "      <td>0.877223</td>\n",
              "      <td>0.896277</td>\n",
              "      <td>0.974221</td>\n",
              "      <td>0.933625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>0.290900</td>\n",
              "      <td>0.388836</td>\n",
              "      <td>0.877368</td>\n",
              "      <td>0.896412</td>\n",
              "      <td>0.974221</td>\n",
              "      <td>0.933698</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [109/109 00:15]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 results: {'eval_loss': 0.3766857087612152, 'eval_accuracy': 0.8822848879248012, 'eval_precision': 0.8974128906834156, 'eval_recall': 0.9791156795562082, 'eval_f1': 0.9364856429463171, 'eval_runtime': 16.0082, 'eval_samples_per_second': 431.965, 'eval_steps_per_second': 6.809, 'epoch': 3.0}\n",
            "Fold 0 results: {'eval_loss': 0.3766857087612152, 'eval_accuracy': 0.8822848879248012, 'eval_precision': 0.8974128906834156, 'eval_recall': 0.9791156795562082, 'eval_f1': 0.9364856429463171, 'eval_runtime': 16.0082, 'eval_samples_per_second': 431.965, 'eval_steps_per_second': 6.809, 'epoch': 3.0}\n",
            "Training on fold 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1410' max='1410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1410/1410 42:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.320900</td>\n",
              "      <td>0.274378</td>\n",
              "      <td>0.891683</td>\n",
              "      <td>0.895574</td>\n",
              "      <td>0.992229</td>\n",
              "      <td>0.941427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.310700</td>\n",
              "      <td>0.309171</td>\n",
              "      <td>0.886835</td>\n",
              "      <td>0.893816</td>\n",
              "      <td>0.988430</td>\n",
              "      <td>0.938745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.308600</td>\n",
              "      <td>0.289133</td>\n",
              "      <td>0.881988</td>\n",
              "      <td>0.903413</td>\n",
              "      <td>0.969090</td>\n",
              "      <td>0.935100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.327900</td>\n",
              "      <td>0.279445</td>\n",
              "      <td>0.888350</td>\n",
              "      <td>0.891783</td>\n",
              "      <td>0.993265</td>\n",
              "      <td>0.939793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.333000</td>\n",
              "      <td>0.280418</td>\n",
              "      <td>0.886078</td>\n",
              "      <td>0.904609</td>\n",
              "      <td>0.972716</td>\n",
              "      <td>0.937427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.323700</td>\n",
              "      <td>0.292696</td>\n",
              "      <td>0.886532</td>\n",
              "      <td>0.898388</td>\n",
              "      <td>0.981696</td>\n",
              "      <td>0.938196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.288200</td>\n",
              "      <td>0.289932</td>\n",
              "      <td>0.889562</td>\n",
              "      <td>0.895716</td>\n",
              "      <td>0.989294</td>\n",
              "      <td>0.940182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.337000</td>\n",
              "      <td>0.282612</td>\n",
              "      <td>0.883654</td>\n",
              "      <td>0.898967</td>\n",
              "      <td>0.977206</td>\n",
              "      <td>0.936455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.326900</td>\n",
              "      <td>0.283990</td>\n",
              "      <td>0.881836</td>\n",
              "      <td>0.900048</td>\n",
              "      <td>0.973407</td>\n",
              "      <td>0.935291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.305800</td>\n",
              "      <td>0.285200</td>\n",
              "      <td>0.883805</td>\n",
              "      <td>0.896590</td>\n",
              "      <td>0.980660</td>\n",
              "      <td>0.936742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.340100</td>\n",
              "      <td>0.285293</td>\n",
              "      <td>0.881988</td>\n",
              "      <td>0.895893</td>\n",
              "      <td>0.979278</td>\n",
              "      <td>0.935731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.292600</td>\n",
              "      <td>0.290690</td>\n",
              "      <td>0.871838</td>\n",
              "      <td>0.903278</td>\n",
              "      <td>0.956312</td>\n",
              "      <td>0.929039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.297300</td>\n",
              "      <td>0.294037</td>\n",
              "      <td>0.879109</td>\n",
              "      <td>0.894828</td>\n",
              "      <td>0.977033</td>\n",
              "      <td>0.934126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.295082</td>\n",
              "      <td>0.871686</td>\n",
              "      <td>0.901821</td>\n",
              "      <td>0.958038</td>\n",
              "      <td>0.929080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.322800</td>\n",
              "      <td>0.301402</td>\n",
              "      <td>0.885472</td>\n",
              "      <td>0.887487</td>\n",
              "      <td>0.995683</td>\n",
              "      <td>0.938477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.338400</td>\n",
              "      <td>0.304501</td>\n",
              "      <td>0.874110</td>\n",
              "      <td>0.904700</td>\n",
              "      <td>0.957348</td>\n",
              "      <td>0.930279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.346100</td>\n",
              "      <td>0.294874</td>\n",
              "      <td>0.885472</td>\n",
              "      <td>0.898528</td>\n",
              "      <td>0.980142</td>\n",
              "      <td>0.937562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.362800</td>\n",
              "      <td>0.301587</td>\n",
              "      <td>0.879261</td>\n",
              "      <td>0.902742</td>\n",
              "      <td>0.966500</td>\n",
              "      <td>0.933533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.317300</td>\n",
              "      <td>0.290104</td>\n",
              "      <td>0.884411</td>\n",
              "      <td>0.891345</td>\n",
              "      <td>0.988776</td>\n",
              "      <td>0.937536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.332900</td>\n",
              "      <td>0.292998</td>\n",
              "      <td>0.883048</td>\n",
              "      <td>0.893400</td>\n",
              "      <td>0.984113</td>\n",
              "      <td>0.936565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.301000</td>\n",
              "      <td>0.296061</td>\n",
              "      <td>0.882291</td>\n",
              "      <td>0.896677</td>\n",
              "      <td>0.978587</td>\n",
              "      <td>0.935843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.291200</td>\n",
              "      <td>0.319075</td>\n",
              "      <td>0.873201</td>\n",
              "      <td>0.883437</td>\n",
              "      <td>0.985495</td>\n",
              "      <td>0.931679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.295900</td>\n",
              "      <td>0.327488</td>\n",
              "      <td>0.862445</td>\n",
              "      <td>0.899918</td>\n",
              "      <td>0.948714</td>\n",
              "      <td>0.923672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.306026</td>\n",
              "      <td>0.885017</td>\n",
              "      <td>0.889233</td>\n",
              "      <td>0.992575</td>\n",
              "      <td>0.938066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.290600</td>\n",
              "      <td>0.304843</td>\n",
              "      <td>0.880624</td>\n",
              "      <td>0.897000</td>\n",
              "      <td>0.975997</td>\n",
              "      <td>0.934833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.324700</td>\n",
              "      <td>0.321883</td>\n",
              "      <td>0.870020</td>\n",
              "      <td>0.887388</td>\n",
              "      <td>0.975652</td>\n",
              "      <td>0.929429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.326600</td>\n",
              "      <td>0.315705</td>\n",
              "      <td>0.872595</td>\n",
              "      <td>0.883364</td>\n",
              "      <td>0.984804</td>\n",
              "      <td>0.931330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.270800</td>\n",
              "      <td>0.320570</td>\n",
              "      <td>0.877897</td>\n",
              "      <td>0.881408</td>\n",
              "      <td>0.994647</td>\n",
              "      <td>0.934610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.310400</td>\n",
              "      <td>0.308565</td>\n",
              "      <td>0.870171</td>\n",
              "      <td>0.905357</td>\n",
              "      <td>0.951476</td>\n",
              "      <td>0.927844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.331600</td>\n",
              "      <td>0.307413</td>\n",
              "      <td>0.874564</td>\n",
              "      <td>0.892084</td>\n",
              "      <td>0.974961</td>\n",
              "      <td>0.931683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.300200</td>\n",
              "      <td>0.325836</td>\n",
              "      <td>0.875019</td>\n",
              "      <td>0.883297</td>\n",
              "      <td>0.988085</td>\n",
              "      <td>0.932757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.301800</td>\n",
              "      <td>0.305945</td>\n",
              "      <td>0.872444</td>\n",
              "      <td>0.889624</td>\n",
              "      <td>0.975652</td>\n",
              "      <td>0.930654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.285400</td>\n",
              "      <td>0.317587</td>\n",
              "      <td>0.867141</td>\n",
              "      <td>0.893498</td>\n",
              "      <td>0.963391</td>\n",
              "      <td>0.927129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.333200</td>\n",
              "      <td>0.322093</td>\n",
              "      <td>0.858052</td>\n",
              "      <td>0.888320</td>\n",
              "      <td>0.958729</td>\n",
              "      <td>0.922183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.311400</td>\n",
              "      <td>0.320370</td>\n",
              "      <td>0.873807</td>\n",
              "      <td>0.881855</td>\n",
              "      <td>0.988603</td>\n",
              "      <td>0.932183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.263400</td>\n",
              "      <td>0.323356</td>\n",
              "      <td>0.874261</td>\n",
              "      <td>0.879339</td>\n",
              "      <td>0.992920</td>\n",
              "      <td>0.932685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.301700</td>\n",
              "      <td>0.309882</td>\n",
              "      <td>0.874867</td>\n",
              "      <td>0.880227</td>\n",
              "      <td>0.992402</td>\n",
              "      <td>0.932955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.302500</td>\n",
              "      <td>0.308586</td>\n",
              "      <td>0.881382</td>\n",
              "      <td>0.888458</td>\n",
              "      <td>0.988948</td>\n",
              "      <td>0.936014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.281300</td>\n",
              "      <td>0.315438</td>\n",
              "      <td>0.881685</td>\n",
              "      <td>0.894240</td>\n",
              "      <td>0.981178</td>\n",
              "      <td>0.935694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.341800</td>\n",
              "      <td>0.326505</td>\n",
              "      <td>0.863354</td>\n",
              "      <td>0.900016</td>\n",
              "      <td>0.949750</td>\n",
              "      <td>0.924214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.301500</td>\n",
              "      <td>0.313832</td>\n",
              "      <td>0.876988</td>\n",
              "      <td>0.886149</td>\n",
              "      <td>0.986531</td>\n",
              "      <td>0.933649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.332000</td>\n",
              "      <td>0.318603</td>\n",
              "      <td>0.874564</td>\n",
              "      <td>0.894452</td>\n",
              "      <td>0.971680</td>\n",
              "      <td>0.931468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.305100</td>\n",
              "      <td>0.319491</td>\n",
              "      <td>0.879867</td>\n",
              "      <td>0.888164</td>\n",
              "      <td>0.987394</td>\n",
              "      <td>0.935154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.234800</td>\n",
              "      <td>0.319566</td>\n",
              "      <td>0.878503</td>\n",
              "      <td>0.886445</td>\n",
              "      <td>0.988085</td>\n",
              "      <td>0.934509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.312700</td>\n",
              "      <td>0.317916</td>\n",
              "      <td>0.876231</td>\n",
              "      <td>0.886660</td>\n",
              "      <td>0.984804</td>\n",
              "      <td>0.933159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.288200</td>\n",
              "      <td>0.319260</td>\n",
              "      <td>0.874867</td>\n",
              "      <td>0.887104</td>\n",
              "      <td>0.982386</td>\n",
              "      <td>0.932317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.336400</td>\n",
              "      <td>0.337692</td>\n",
              "      <td>0.868353</td>\n",
              "      <td>0.884051</td>\n",
              "      <td>0.978242</td>\n",
              "      <td>0.928765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.262700</td>\n",
              "      <td>0.355227</td>\n",
              "      <td>0.868808</td>\n",
              "      <td>0.881842</td>\n",
              "      <td>0.982041</td>\n",
              "      <td>0.929248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.294300</td>\n",
              "      <td>0.309999</td>\n",
              "      <td>0.878352</td>\n",
              "      <td>0.891769</td>\n",
              "      <td>0.980314</td>\n",
              "      <td>0.933948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.271900</td>\n",
              "      <td>0.316047</td>\n",
              "      <td>0.880776</td>\n",
              "      <td>0.890084</td>\n",
              "      <td>0.985840</td>\n",
              "      <td>0.935518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.289500</td>\n",
              "      <td>0.321403</td>\n",
              "      <td>0.871535</td>\n",
              "      <td>0.900113</td>\n",
              "      <td>0.960111</td>\n",
              "      <td>0.929144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.286200</td>\n",
              "      <td>0.336237</td>\n",
              "      <td>0.857446</td>\n",
              "      <td>0.904977</td>\n",
              "      <td>0.935762</td>\n",
              "      <td>0.920112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.263700</td>\n",
              "      <td>0.332490</td>\n",
              "      <td>0.875170</td>\n",
              "      <td>0.885577</td>\n",
              "      <td>0.984977</td>\n",
              "      <td>0.932636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.299500</td>\n",
              "      <td>0.329533</td>\n",
              "      <td>0.875776</td>\n",
              "      <td>0.883269</td>\n",
              "      <td>0.989121</td>\n",
              "      <td>0.933203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.318200</td>\n",
              "      <td>0.329601</td>\n",
              "      <td>0.875019</td>\n",
              "      <td>0.886760</td>\n",
              "      <td>0.983077</td>\n",
              "      <td>0.932438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.257200</td>\n",
              "      <td>0.343269</td>\n",
              "      <td>0.869262</td>\n",
              "      <td>0.883920</td>\n",
              "      <td>0.979624</td>\n",
              "      <td>0.929314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.324768</td>\n",
              "      <td>0.868656</td>\n",
              "      <td>0.891664</td>\n",
              "      <td>0.967881</td>\n",
              "      <td>0.928211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.270400</td>\n",
              "      <td>0.315890</td>\n",
              "      <td>0.877140</td>\n",
              "      <td>0.882841</td>\n",
              "      <td>0.991539</td>\n",
              "      <td>0.934038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.308000</td>\n",
              "      <td>0.320867</td>\n",
              "      <td>0.876988</td>\n",
              "      <td>0.881767</td>\n",
              "      <td>0.992920</td>\n",
              "      <td>0.934048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.337091</td>\n",
              "      <td>0.855628</td>\n",
              "      <td>0.888782</td>\n",
              "      <td>0.954930</td>\n",
              "      <td>0.920669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.269500</td>\n",
              "      <td>0.347480</td>\n",
              "      <td>0.873807</td>\n",
              "      <td>0.885537</td>\n",
              "      <td>0.983250</td>\n",
              "      <td>0.931839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.306200</td>\n",
              "      <td>0.333704</td>\n",
              "      <td>0.858506</td>\n",
              "      <td>0.895973</td>\n",
              "      <td>0.948886</td>\n",
              "      <td>0.921671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.345500</td>\n",
              "      <td>0.354703</td>\n",
              "      <td>0.853204</td>\n",
              "      <td>0.894987</td>\n",
              "      <td>0.943360</td>\n",
              "      <td>0.918537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.274500</td>\n",
              "      <td>0.358803</td>\n",
              "      <td>0.875473</td>\n",
              "      <td>0.878331</td>\n",
              "      <td>0.996028</td>\n",
              "      <td>0.933484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.267600</td>\n",
              "      <td>0.324633</td>\n",
              "      <td>0.877291</td>\n",
              "      <td>0.882507</td>\n",
              "      <td>0.992229</td>\n",
              "      <td>0.934157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.285900</td>\n",
              "      <td>0.328077</td>\n",
              "      <td>0.876079</td>\n",
              "      <td>0.883778</td>\n",
              "      <td>0.988776</td>\n",
              "      <td>0.933333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.330652</td>\n",
              "      <td>0.876231</td>\n",
              "      <td>0.883441</td>\n",
              "      <td>0.989466</td>\n",
              "      <td>0.933453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.275500</td>\n",
              "      <td>0.329390</td>\n",
              "      <td>0.875473</td>\n",
              "      <td>0.882997</td>\n",
              "      <td>0.989121</td>\n",
              "      <td>0.933051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.250300</td>\n",
              "      <td>0.328541</td>\n",
              "      <td>0.870323</td>\n",
              "      <td>0.893603</td>\n",
              "      <td>0.967363</td>\n",
              "      <td>0.929022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.278900</td>\n",
              "      <td>0.345878</td>\n",
              "      <td>0.872595</td>\n",
              "      <td>0.891862</td>\n",
              "      <td>0.972716</td>\n",
              "      <td>0.930536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.271300</td>\n",
              "      <td>0.336434</td>\n",
              "      <td>0.867596</td>\n",
              "      <td>0.892418</td>\n",
              "      <td>0.965464</td>\n",
              "      <td>0.927505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.284000</td>\n",
              "      <td>0.335956</td>\n",
              "      <td>0.869868</td>\n",
              "      <td>0.889820</td>\n",
              "      <td>0.972026</td>\n",
              "      <td>0.929108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>0.321174</td>\n",
              "      <td>0.864717</td>\n",
              "      <td>0.889720</td>\n",
              "      <td>0.965464</td>\n",
              "      <td>0.926046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.316300</td>\n",
              "      <td>0.320894</td>\n",
              "      <td>0.869717</td>\n",
              "      <td>0.889064</td>\n",
              "      <td>0.972889</td>\n",
              "      <td>0.929090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.278700</td>\n",
              "      <td>0.323568</td>\n",
              "      <td>0.874867</td>\n",
              "      <td>0.889290</td>\n",
              "      <td>0.979278</td>\n",
              "      <td>0.932117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.297500</td>\n",
              "      <td>0.324546</td>\n",
              "      <td>0.874261</td>\n",
              "      <td>0.887760</td>\n",
              "      <td>0.980660</td>\n",
              "      <td>0.931900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.282300</td>\n",
              "      <td>0.334666</td>\n",
              "      <td>0.873504</td>\n",
              "      <td>0.887915</td>\n",
              "      <td>0.979451</td>\n",
              "      <td>0.931439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.286200</td>\n",
              "      <td>0.328738</td>\n",
              "      <td>0.874716</td>\n",
              "      <td>0.887087</td>\n",
              "      <td>0.982214</td>\n",
              "      <td>0.932230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.285200</td>\n",
              "      <td>0.318345</td>\n",
              "      <td>0.875019</td>\n",
              "      <td>0.892755</td>\n",
              "      <td>0.974616</td>\n",
              "      <td>0.931891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.313200</td>\n",
              "      <td>0.314875</td>\n",
              "      <td>0.873807</td>\n",
              "      <td>0.894494</td>\n",
              "      <td>0.970644</td>\n",
              "      <td>0.931014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.251400</td>\n",
              "      <td>0.316324</td>\n",
              "      <td>0.877897</td>\n",
              "      <td>0.881875</td>\n",
              "      <td>0.993956</td>\n",
              "      <td>0.934567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.256900</td>\n",
              "      <td>0.312078</td>\n",
              "      <td>0.877897</td>\n",
              "      <td>0.883875</td>\n",
              "      <td>0.991021</td>\n",
              "      <td>0.934386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.235500</td>\n",
              "      <td>0.314674</td>\n",
              "      <td>0.871232</td>\n",
              "      <td>0.889976</td>\n",
              "      <td>0.973580</td>\n",
              "      <td>0.929903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.317000</td>\n",
              "      <td>0.315582</td>\n",
              "      <td>0.869717</td>\n",
              "      <td>0.890172</td>\n",
              "      <td>0.971335</td>\n",
              "      <td>0.928984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.307700</td>\n",
              "      <td>0.316909</td>\n",
              "      <td>0.876382</td>\n",
              "      <td>0.883696</td>\n",
              "      <td>0.989294</td>\n",
              "      <td>0.933518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.278900</td>\n",
              "      <td>0.328903</td>\n",
              "      <td>0.872595</td>\n",
              "      <td>0.879834</td>\n",
              "      <td>0.989984</td>\n",
              "      <td>0.931665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.260400</td>\n",
              "      <td>0.322675</td>\n",
              "      <td>0.872747</td>\n",
              "      <td>0.892376</td>\n",
              "      <td>0.972198</td>\n",
              "      <td>0.930579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.284800</td>\n",
              "      <td>0.327417</td>\n",
              "      <td>0.871989</td>\n",
              "      <td>0.890309</td>\n",
              "      <td>0.974098</td>\n",
              "      <td>0.930321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.328667</td>\n",
              "      <td>0.875170</td>\n",
              "      <td>0.885936</td>\n",
              "      <td>0.984459</td>\n",
              "      <td>0.932603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.302100</td>\n",
              "      <td>0.323682</td>\n",
              "      <td>0.875625</td>\n",
              "      <td>0.883606</td>\n",
              "      <td>0.988430</td>\n",
              "      <td>0.933083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.285700</td>\n",
              "      <td>0.320306</td>\n",
              "      <td>0.874867</td>\n",
              "      <td>0.883043</td>\n",
              "      <td>0.988258</td>\n",
              "      <td>0.932692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.231200</td>\n",
              "      <td>0.320255</td>\n",
              "      <td>0.876231</td>\n",
              "      <td>0.884508</td>\n",
              "      <td>0.987912</td>\n",
              "      <td>0.933355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.324953</td>\n",
              "      <td>0.871232</td>\n",
              "      <td>0.893079</td>\n",
              "      <td>0.969263</td>\n",
              "      <td>0.929612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.230700</td>\n",
              "      <td>0.361316</td>\n",
              "      <td>0.873504</td>\n",
              "      <td>0.884305</td>\n",
              "      <td>0.984631</td>\n",
              "      <td>0.931775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.288700</td>\n",
              "      <td>0.327866</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.890279</td>\n",
              "      <td>0.970989</td>\n",
              "      <td>0.928884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.260800</td>\n",
              "      <td>0.324177</td>\n",
              "      <td>0.872898</td>\n",
              "      <td>0.887845</td>\n",
              "      <td>0.978760</td>\n",
              "      <td>0.931088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.240500</td>\n",
              "      <td>0.331495</td>\n",
              "      <td>0.873353</td>\n",
              "      <td>0.885843</td>\n",
              "      <td>0.982214</td>\n",
              "      <td>0.931543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.275600</td>\n",
              "      <td>0.323296</td>\n",
              "      <td>0.873504</td>\n",
              "      <td>0.882880</td>\n",
              "      <td>0.986704</td>\n",
              "      <td>0.931909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.301600</td>\n",
              "      <td>0.318615</td>\n",
              "      <td>0.876837</td>\n",
              "      <td>0.899262</td>\n",
              "      <td>0.968054</td>\n",
              "      <td>0.932391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.307500</td>\n",
              "      <td>0.317765</td>\n",
              "      <td>0.873504</td>\n",
              "      <td>0.889133</td>\n",
              "      <td>0.977724</td>\n",
              "      <td>0.931327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.279800</td>\n",
              "      <td>0.328753</td>\n",
              "      <td>0.876231</td>\n",
              "      <td>0.883796</td>\n",
              "      <td>0.988948</td>\n",
              "      <td>0.933420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.290400</td>\n",
              "      <td>0.328743</td>\n",
              "      <td>0.873807</td>\n",
              "      <td>0.884580</td>\n",
              "      <td>0.984631</td>\n",
              "      <td>0.931928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.229700</td>\n",
              "      <td>0.349594</td>\n",
              "      <td>0.875170</td>\n",
              "      <td>0.880147</td>\n",
              "      <td>0.992920</td>\n",
              "      <td>0.933139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.240500</td>\n",
              "      <td>0.348770</td>\n",
              "      <td>0.875170</td>\n",
              "      <td>0.887744</td>\n",
              "      <td>0.981868</td>\n",
              "      <td>0.932437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.252200</td>\n",
              "      <td>0.338674</td>\n",
              "      <td>0.866384</td>\n",
              "      <td>0.888678</td>\n",
              "      <td>0.969090</td>\n",
              "      <td>0.927144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.272500</td>\n",
              "      <td>0.336259</td>\n",
              "      <td>0.868050</td>\n",
              "      <td>0.894864</td>\n",
              "      <td>0.962701</td>\n",
              "      <td>0.927543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.267400</td>\n",
              "      <td>0.333446</td>\n",
              "      <td>0.863657</td>\n",
              "      <td>0.890094</td>\n",
              "      <td>0.963564</td>\n",
              "      <td>0.925373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.284400</td>\n",
              "      <td>0.329371</td>\n",
              "      <td>0.864111</td>\n",
              "      <td>0.891019</td>\n",
              "      <td>0.962873</td>\n",
              "      <td>0.925554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.222700</td>\n",
              "      <td>0.357540</td>\n",
              "      <td>0.872444</td>\n",
              "      <td>0.883346</td>\n",
              "      <td>0.984631</td>\n",
              "      <td>0.931243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.290500</td>\n",
              "      <td>0.343799</td>\n",
              "      <td>0.870929</td>\n",
              "      <td>0.885679</td>\n",
              "      <td>0.979278</td>\n",
              "      <td>0.930130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.248600</td>\n",
              "      <td>0.334030</td>\n",
              "      <td>0.867596</td>\n",
              "      <td>0.893927</td>\n",
              "      <td>0.963391</td>\n",
              "      <td>0.927360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.264400</td>\n",
              "      <td>0.357260</td>\n",
              "      <td>0.872444</td>\n",
              "      <td>0.888401</td>\n",
              "      <td>0.977379</td>\n",
              "      <td>0.930768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>0.248000</td>\n",
              "      <td>0.346243</td>\n",
              "      <td>0.875322</td>\n",
              "      <td>0.888976</td>\n",
              "      <td>0.980314</td>\n",
              "      <td>0.932414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.282800</td>\n",
              "      <td>0.325429</td>\n",
              "      <td>0.871383</td>\n",
              "      <td>0.890240</td>\n",
              "      <td>0.973407</td>\n",
              "      <td>0.929968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.273800</td>\n",
              "      <td>0.321235</td>\n",
              "      <td>0.867444</td>\n",
              "      <td>0.892401</td>\n",
              "      <td>0.965291</td>\n",
              "      <td>0.927416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.262100</td>\n",
              "      <td>0.330706</td>\n",
              "      <td>0.869262</td>\n",
              "      <td>0.892232</td>\n",
              "      <td>0.967881</td>\n",
              "      <td>0.928518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.257600</td>\n",
              "      <td>0.341441</td>\n",
              "      <td>0.871989</td>\n",
              "      <td>0.890186</td>\n",
              "      <td>0.974270</td>\n",
              "      <td>0.930332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.270200</td>\n",
              "      <td>0.338930</td>\n",
              "      <td>0.873656</td>\n",
              "      <td>0.891363</td>\n",
              "      <td>0.974788</td>\n",
              "      <td>0.931211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.283000</td>\n",
              "      <td>0.326210</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.891022</td>\n",
              "      <td>0.969953</td>\n",
              "      <td>0.928814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.295400</td>\n",
              "      <td>0.322007</td>\n",
              "      <td>0.868808</td>\n",
              "      <td>0.891931</td>\n",
              "      <td>0.967709</td>\n",
              "      <td>0.928276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.289400</td>\n",
              "      <td>0.322766</td>\n",
              "      <td>0.869414</td>\n",
              "      <td>0.891004</td>\n",
              "      <td>0.969781</td>\n",
              "      <td>0.928725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.256100</td>\n",
              "      <td>0.337310</td>\n",
              "      <td>0.872444</td>\n",
              "      <td>0.884897</td>\n",
              "      <td>0.982386</td>\n",
              "      <td>0.931097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.255300</td>\n",
              "      <td>0.345397</td>\n",
              "      <td>0.873807</td>\n",
              "      <td>0.885057</td>\n",
              "      <td>0.983941</td>\n",
              "      <td>0.931883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.246700</td>\n",
              "      <td>0.330133</td>\n",
              "      <td>0.870474</td>\n",
              "      <td>0.890259</td>\n",
              "      <td>0.972198</td>\n",
              "      <td>0.929426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.280600</td>\n",
              "      <td>0.324314</td>\n",
              "      <td>0.867141</td>\n",
              "      <td>0.891866</td>\n",
              "      <td>0.965636</td>\n",
              "      <td>0.927286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>0.216700</td>\n",
              "      <td>0.329683</td>\n",
              "      <td>0.868808</td>\n",
              "      <td>0.890935</td>\n",
              "      <td>0.969090</td>\n",
              "      <td>0.928371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>0.227400</td>\n",
              "      <td>0.341584</td>\n",
              "      <td>0.869717</td>\n",
              "      <td>0.890172</td>\n",
              "      <td>0.971335</td>\n",
              "      <td>0.928984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>0.249300</td>\n",
              "      <td>0.355228</td>\n",
              "      <td>0.873958</td>\n",
              "      <td>0.887482</td>\n",
              "      <td>0.980660</td>\n",
              "      <td>0.931747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>0.255600</td>\n",
              "      <td>0.341623</td>\n",
              "      <td>0.868656</td>\n",
              "      <td>0.890670</td>\n",
              "      <td>0.969263</td>\n",
              "      <td>0.928306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.258000</td>\n",
              "      <td>0.334992</td>\n",
              "      <td>0.868505</td>\n",
              "      <td>0.890900</td>\n",
              "      <td>0.968745</td>\n",
              "      <td>0.928193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>0.261400</td>\n",
              "      <td>0.340233</td>\n",
              "      <td>0.871686</td>\n",
              "      <td>0.887582</td>\n",
              "      <td>0.977551</td>\n",
              "      <td>0.930397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>0.271300</td>\n",
              "      <td>0.340892</td>\n",
              "      <td>0.873353</td>\n",
              "      <td>0.887533</td>\n",
              "      <td>0.979796</td>\n",
              "      <td>0.931385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>0.185900</td>\n",
              "      <td>0.343724</td>\n",
              "      <td>0.874110</td>\n",
              "      <td>0.887500</td>\n",
              "      <td>0.980832</td>\n",
              "      <td>0.931835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>0.267800</td>\n",
              "      <td>0.342074</td>\n",
              "      <td>0.874110</td>\n",
              "      <td>0.887500</td>\n",
              "      <td>0.980832</td>\n",
              "      <td>0.931835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.232700</td>\n",
              "      <td>0.340204</td>\n",
              "      <td>0.873050</td>\n",
              "      <td>0.887377</td>\n",
              "      <td>0.979624</td>\n",
              "      <td>0.931221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>0.246800</td>\n",
              "      <td>0.340981</td>\n",
              "      <td>0.871535</td>\n",
              "      <td>0.887322</td>\n",
              "      <td>0.977724</td>\n",
              "      <td>0.930332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>0.227800</td>\n",
              "      <td>0.341828</td>\n",
              "      <td>0.872292</td>\n",
              "      <td>0.887289</td>\n",
              "      <td>0.978760</td>\n",
              "      <td>0.930782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.265900</td>\n",
              "      <td>0.341339</td>\n",
              "      <td>0.872292</td>\n",
              "      <td>0.887289</td>\n",
              "      <td>0.978760</td>\n",
              "      <td>0.930782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>0.252800</td>\n",
              "      <td>0.341528</td>\n",
              "      <td>0.873353</td>\n",
              "      <td>0.888262</td>\n",
              "      <td>0.978760</td>\n",
              "      <td>0.931318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.220100</td>\n",
              "      <td>0.342404</td>\n",
              "      <td>0.873353</td>\n",
              "      <td>0.888262</td>\n",
              "      <td>0.978760</td>\n",
              "      <td>0.931318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>0.243800</td>\n",
              "      <td>0.342711</td>\n",
              "      <td>0.874413</td>\n",
              "      <td>0.888384</td>\n",
              "      <td>0.979969</td>\n",
              "      <td>0.931932</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/104 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 results: {'eval_loss': 0.31604668498039246, 'eval_accuracy': 0.8807756400545372, 'eval_precision': 0.8900841908325537, 'eval_recall': 0.9858400967017786, 'eval_f1': 0.9355182302335109, 'eval_runtime': 15.1301, 'eval_samples_per_second': 436.281, 'eval_steps_per_second': 6.874, 'epoch': 3.0}\n",
            "Fold 1 results: {'eval_loss': 0.31604668498039246, 'eval_accuracy': 0.8807756400545372, 'eval_precision': 0.8900841908325537, 'eval_recall': 0.9858400967017786, 'eval_f1': 0.9355182302335109, 'eval_runtime': 15.1301, 'eval_samples_per_second': 436.281, 'eval_steps_per_second': 6.874, 'epoch': 3.0}\n",
            "Training on fold 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1332' max='1332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1332/1332 51:09, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.310900</td>\n",
              "      <td>0.324574</td>\n",
              "      <td>0.852545</td>\n",
              "      <td>0.869577</td>\n",
              "      <td>0.962741</td>\n",
              "      <td>0.913791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.286800</td>\n",
              "      <td>0.333022</td>\n",
              "      <td>0.847808</td>\n",
              "      <td>0.850232</td>\n",
              "      <td>0.986234</td>\n",
              "      <td>0.913197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.279600</td>\n",
              "      <td>0.327028</td>\n",
              "      <td>0.854974</td>\n",
              "      <td>0.877044</td>\n",
              "      <td>0.955260</td>\n",
              "      <td>0.914482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.341900</td>\n",
              "      <td>0.328090</td>\n",
              "      <td>0.852423</td>\n",
              "      <td>0.889459</td>\n",
              "      <td>0.934311</td>\n",
              "      <td>0.911333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.266700</td>\n",
              "      <td>0.355915</td>\n",
              "      <td>0.850601</td>\n",
              "      <td>0.852489</td>\n",
              "      <td>0.986683</td>\n",
              "      <td>0.914690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.256300</td>\n",
              "      <td>0.325248</td>\n",
              "      <td>0.854974</td>\n",
              "      <td>0.871531</td>\n",
              "      <td>0.963340</td>\n",
              "      <td>0.915139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.227100</td>\n",
              "      <td>0.336762</td>\n",
              "      <td>0.851694</td>\n",
              "      <td>0.856155</td>\n",
              "      <td>0.982343</td>\n",
              "      <td>0.914919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.286800</td>\n",
              "      <td>0.325416</td>\n",
              "      <td>0.857525</td>\n",
              "      <td>0.871795</td>\n",
              "      <td>0.966632</td>\n",
              "      <td>0.916767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.302800</td>\n",
              "      <td>0.326495</td>\n",
              "      <td>0.855095</td>\n",
              "      <td>0.877579</td>\n",
              "      <td>0.954661</td>\n",
              "      <td>0.914499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.241400</td>\n",
              "      <td>0.361683</td>\n",
              "      <td>0.849387</td>\n",
              "      <td>0.854963</td>\n",
              "      <td>0.980847</td>\n",
              "      <td>0.913589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.266700</td>\n",
              "      <td>0.333514</td>\n",
              "      <td>0.861897</td>\n",
              "      <td>0.882061</td>\n",
              "      <td>0.957953</td>\n",
              "      <td>0.918442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.290500</td>\n",
              "      <td>0.331355</td>\n",
              "      <td>0.858982</td>\n",
              "      <td>0.875749</td>\n",
              "      <td>0.962891</td>\n",
              "      <td>0.917255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.291300</td>\n",
              "      <td>0.325065</td>\n",
              "      <td>0.863841</td>\n",
              "      <td>0.885714</td>\n",
              "      <td>0.955559</td>\n",
              "      <td>0.919312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.264300</td>\n",
              "      <td>0.355205</td>\n",
              "      <td>0.854002</td>\n",
              "      <td>0.865839</td>\n",
              "      <td>0.970522</td>\n",
              "      <td>0.915197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.250100</td>\n",
              "      <td>0.338603</td>\n",
              "      <td>0.855338</td>\n",
              "      <td>0.863612</td>\n",
              "      <td>0.975909</td>\n",
              "      <td>0.916333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.254100</td>\n",
              "      <td>0.348947</td>\n",
              "      <td>0.859954</td>\n",
              "      <td>0.881801</td>\n",
              "      <td>0.955559</td>\n",
              "      <td>0.917199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.287400</td>\n",
              "      <td>0.332425</td>\n",
              "      <td>0.858618</td>\n",
              "      <td>0.877858</td>\n",
              "      <td>0.959300</td>\n",
              "      <td>0.916774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.264100</td>\n",
              "      <td>0.340054</td>\n",
              "      <td>0.850723</td>\n",
              "      <td>0.859478</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.913869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.244100</td>\n",
              "      <td>0.373365</td>\n",
              "      <td>0.849872</td>\n",
              "      <td>0.850019</td>\n",
              "      <td>0.989675</td>\n",
              "      <td>0.914546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.254700</td>\n",
              "      <td>0.370813</td>\n",
              "      <td>0.843556</td>\n",
              "      <td>0.887182</td>\n",
              "      <td>0.924884</td>\n",
              "      <td>0.905641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.239400</td>\n",
              "      <td>0.364061</td>\n",
              "      <td>0.849387</td>\n",
              "      <td>0.867721</td>\n",
              "      <td>0.960946</td>\n",
              "      <td>0.911957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.292400</td>\n",
              "      <td>0.348336</td>\n",
              "      <td>0.847443</td>\n",
              "      <td>0.859261</td>\n",
              "      <td>0.971121</td>\n",
              "      <td>0.911773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.271000</td>\n",
              "      <td>0.349759</td>\n",
              "      <td>0.845864</td>\n",
              "      <td>0.871025</td>\n",
              "      <td>0.950920</td>\n",
              "      <td>0.909221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.265900</td>\n",
              "      <td>0.367602</td>\n",
              "      <td>0.849630</td>\n",
              "      <td>0.863533</td>\n",
              "      <td>0.967679</td>\n",
              "      <td>0.912645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.276700</td>\n",
              "      <td>0.343834</td>\n",
              "      <td>0.850723</td>\n",
              "      <td>0.861001</td>\n",
              "      <td>0.973216</td>\n",
              "      <td>0.913676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.258800</td>\n",
              "      <td>0.349895</td>\n",
              "      <td>0.852302</td>\n",
              "      <td>0.873378</td>\n",
              "      <td>0.956756</td>\n",
              "      <td>0.913168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.287000</td>\n",
              "      <td>0.358980</td>\n",
              "      <td>0.842463</td>\n",
              "      <td>0.876643</td>\n",
              "      <td>0.937902</td>\n",
              "      <td>0.906239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.299000</td>\n",
              "      <td>0.346379</td>\n",
              "      <td>0.849751</td>\n",
              "      <td>0.870073</td>\n",
              "      <td>0.957953</td>\n",
              "      <td>0.911901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.284500</td>\n",
              "      <td>0.365029</td>\n",
              "      <td>0.851209</td>\n",
              "      <td>0.859173</td>\n",
              "      <td>0.976807</td>\n",
              "      <td>0.914222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.284300</td>\n",
              "      <td>0.360362</td>\n",
              "      <td>0.852180</td>\n",
              "      <td>0.878218</td>\n",
              "      <td>0.949574</td>\n",
              "      <td>0.912503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.270600</td>\n",
              "      <td>0.364975</td>\n",
              "      <td>0.846836</td>\n",
              "      <td>0.880545</td>\n",
              "      <td>0.938650</td>\n",
              "      <td>0.908670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.261600</td>\n",
              "      <td>0.399293</td>\n",
              "      <td>0.853881</td>\n",
              "      <td>0.861192</td>\n",
              "      <td>0.977555</td>\n",
              "      <td>0.915691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.307900</td>\n",
              "      <td>0.356552</td>\n",
              "      <td>0.853516</td>\n",
              "      <td>0.862956</td>\n",
              "      <td>0.974263</td>\n",
              "      <td>0.915238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.290500</td>\n",
              "      <td>0.357615</td>\n",
              "      <td>0.851937</td>\n",
              "      <td>0.853611</td>\n",
              "      <td>0.986832</td>\n",
              "      <td>0.915400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.281200</td>\n",
              "      <td>0.348032</td>\n",
              "      <td>0.848415</td>\n",
              "      <td>0.858084</td>\n",
              "      <td>0.974413</td>\n",
              "      <td>0.912556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.256500</td>\n",
              "      <td>0.344427</td>\n",
              "      <td>0.851937</td>\n",
              "      <td>0.871499</td>\n",
              "      <td>0.959000</td>\n",
              "      <td>0.913158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.242100</td>\n",
              "      <td>0.361414</td>\n",
              "      <td>0.852423</td>\n",
              "      <td>0.865215</td>\n",
              "      <td>0.969176</td>\n",
              "      <td>0.914249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.308400</td>\n",
              "      <td>0.344672</td>\n",
              "      <td>0.850966</td>\n",
              "      <td>0.874931</td>\n",
              "      <td>0.952566</td>\n",
              "      <td>0.912100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.272600</td>\n",
              "      <td>0.363762</td>\n",
              "      <td>0.840641</td>\n",
              "      <td>0.840152</td>\n",
              "      <td>0.992518</td>\n",
              "      <td>0.910001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.280100</td>\n",
              "      <td>0.340932</td>\n",
              "      <td>0.847443</td>\n",
              "      <td>0.857840</td>\n",
              "      <td>0.973365</td>\n",
              "      <td>0.911959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.306300</td>\n",
              "      <td>0.345933</td>\n",
              "      <td>0.849265</td>\n",
              "      <td>0.862317</td>\n",
              "      <td>0.969026</td>\n",
              "      <td>0.912563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.284400</td>\n",
              "      <td>0.389804</td>\n",
              "      <td>0.847565</td>\n",
              "      <td>0.847592</td>\n",
              "      <td>0.990274</td>\n",
              "      <td>0.913395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.316300</td>\n",
              "      <td>0.339670</td>\n",
              "      <td>0.860683</td>\n",
              "      <td>0.891846</td>\n",
              "      <td>0.942690</td>\n",
              "      <td>0.916564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.303900</td>\n",
              "      <td>0.350609</td>\n",
              "      <td>0.853638</td>\n",
              "      <td>0.867355</td>\n",
              "      <td>0.967679</td>\n",
              "      <td>0.914775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.265500</td>\n",
              "      <td>0.341199</td>\n",
              "      <td>0.857160</td>\n",
              "      <td>0.887108</td>\n",
              "      <td>0.944187</td>\n",
              "      <td>0.914758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.278000</td>\n",
              "      <td>0.350364</td>\n",
              "      <td>0.859225</td>\n",
              "      <td>0.886402</td>\n",
              "      <td>0.948077</td>\n",
              "      <td>0.916203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.223700</td>\n",
              "      <td>0.394967</td>\n",
              "      <td>0.851816</td>\n",
              "      <td>0.863280</td>\n",
              "      <td>0.971270</td>\n",
              "      <td>0.914097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.271800</td>\n",
              "      <td>0.360875</td>\n",
              "      <td>0.839913</td>\n",
              "      <td>0.899003</td>\n",
              "      <td>0.904384</td>\n",
              "      <td>0.901686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.289900</td>\n",
              "      <td>0.366699</td>\n",
              "      <td>0.856067</td>\n",
              "      <td>0.884368</td>\n",
              "      <td>0.946431</td>\n",
              "      <td>0.914348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.302200</td>\n",
              "      <td>0.352423</td>\n",
              "      <td>0.856917</td>\n",
              "      <td>0.885991</td>\n",
              "      <td>0.945384</td>\n",
              "      <td>0.914724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.370824</td>\n",
              "      <td>0.851087</td>\n",
              "      <td>0.862399</td>\n",
              "      <td>0.971570</td>\n",
              "      <td>0.913735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.244800</td>\n",
              "      <td>0.372523</td>\n",
              "      <td>0.854245</td>\n",
              "      <td>0.863932</td>\n",
              "      <td>0.973814</td>\n",
              "      <td>0.915588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.256800</td>\n",
              "      <td>0.357307</td>\n",
              "      <td>0.856796</td>\n",
              "      <td>0.882967</td>\n",
              "      <td>0.949424</td>\n",
              "      <td>0.914990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.250600</td>\n",
              "      <td>0.356024</td>\n",
              "      <td>0.857768</td>\n",
              "      <td>0.886536</td>\n",
              "      <td>0.945833</td>\n",
              "      <td>0.915225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.222000</td>\n",
              "      <td>0.395982</td>\n",
              "      <td>0.849630</td>\n",
              "      <td>0.857612</td>\n",
              "      <td>0.976956</td>\n",
              "      <td>0.913402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.269700</td>\n",
              "      <td>0.351705</td>\n",
              "      <td>0.852545</td>\n",
              "      <td>0.873821</td>\n",
              "      <td>0.956457</td>\n",
              "      <td>0.913273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.260500</td>\n",
              "      <td>0.375190</td>\n",
              "      <td>0.847565</td>\n",
              "      <td>0.852102</td>\n",
              "      <td>0.982792</td>\n",
              "      <td>0.912793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.260600</td>\n",
              "      <td>0.357281</td>\n",
              "      <td>0.856796</td>\n",
              "      <td>0.883180</td>\n",
              "      <td>0.949125</td>\n",
              "      <td>0.914966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.241300</td>\n",
              "      <td>0.372049</td>\n",
              "      <td>0.853759</td>\n",
              "      <td>0.885791</td>\n",
              "      <td>0.941194</td>\n",
              "      <td>0.912652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.223300</td>\n",
              "      <td>0.410069</td>\n",
              "      <td>0.853152</td>\n",
              "      <td>0.863673</td>\n",
              "      <td>0.972617</td>\n",
              "      <td>0.914913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.291100</td>\n",
              "      <td>0.350994</td>\n",
              "      <td>0.854124</td>\n",
              "      <td>0.876304</td>\n",
              "      <td>0.955110</td>\n",
              "      <td>0.914012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.277900</td>\n",
              "      <td>0.351344</td>\n",
              "      <td>0.853638</td>\n",
              "      <td>0.868939</td>\n",
              "      <td>0.965285</td>\n",
              "      <td>0.914581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.284100</td>\n",
              "      <td>0.352079</td>\n",
              "      <td>0.852909</td>\n",
              "      <td>0.869131</td>\n",
              "      <td>0.963938</td>\n",
              "      <td>0.914083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.249500</td>\n",
              "      <td>0.361662</td>\n",
              "      <td>0.850723</td>\n",
              "      <td>0.862248</td>\n",
              "      <td>0.971270</td>\n",
              "      <td>0.913518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.241600</td>\n",
              "      <td>0.365624</td>\n",
              "      <td>0.851937</td>\n",
              "      <td>0.862623</td>\n",
              "      <td>0.972467</td>\n",
              "      <td>0.914258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.247900</td>\n",
              "      <td>0.367343</td>\n",
              "      <td>0.852059</td>\n",
              "      <td>0.863413</td>\n",
              "      <td>0.971420</td>\n",
              "      <td>0.914237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.218700</td>\n",
              "      <td>0.374881</td>\n",
              "      <td>0.854367</td>\n",
              "      <td>0.874897</td>\n",
              "      <td>0.957504</td>\n",
              "      <td>0.914339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.258500</td>\n",
              "      <td>0.354553</td>\n",
              "      <td>0.850115</td>\n",
              "      <td>0.889492</td>\n",
              "      <td>0.931019</td>\n",
              "      <td>0.909782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.249000</td>\n",
              "      <td>0.372031</td>\n",
              "      <td>0.853759</td>\n",
              "      <td>0.860129</td>\n",
              "      <td>0.979051</td>\n",
              "      <td>0.915745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.242500</td>\n",
              "      <td>0.371822</td>\n",
              "      <td>0.847808</td>\n",
              "      <td>0.848345</td>\n",
              "      <td>0.989376</td>\n",
              "      <td>0.913449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.254400</td>\n",
              "      <td>0.362279</td>\n",
              "      <td>0.850723</td>\n",
              "      <td>0.860048</td>\n",
              "      <td>0.974712</td>\n",
              "      <td>0.913797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.249400</td>\n",
              "      <td>0.384540</td>\n",
              "      <td>0.851816</td>\n",
              "      <td>0.859361</td>\n",
              "      <td>0.977405</td>\n",
              "      <td>0.914590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.224600</td>\n",
              "      <td>0.370903</td>\n",
              "      <td>0.829953</td>\n",
              "      <td>0.894666</td>\n",
              "      <td>0.896005</td>\n",
              "      <td>0.895335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.244200</td>\n",
              "      <td>0.382371</td>\n",
              "      <td>0.852666</td>\n",
              "      <td>0.866033</td>\n",
              "      <td>0.968278</td>\n",
              "      <td>0.914306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.299300</td>\n",
              "      <td>0.371640</td>\n",
              "      <td>0.850601</td>\n",
              "      <td>0.858608</td>\n",
              "      <td>0.976807</td>\n",
              "      <td>0.913902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.251800</td>\n",
              "      <td>0.383372</td>\n",
              "      <td>0.847929</td>\n",
              "      <td>0.849981</td>\n",
              "      <td>0.986832</td>\n",
              "      <td>0.913308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.280600</td>\n",
              "      <td>0.363723</td>\n",
              "      <td>0.854002</td>\n",
              "      <td>0.862836</td>\n",
              "      <td>0.975161</td>\n",
              "      <td>0.915566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.225500</td>\n",
              "      <td>0.362262</td>\n",
              "      <td>0.851451</td>\n",
              "      <td>0.875103</td>\n",
              "      <td>0.953015</td>\n",
              "      <td>0.912399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.279900</td>\n",
              "      <td>0.363752</td>\n",
              "      <td>0.848536</td>\n",
              "      <td>0.873762</td>\n",
              "      <td>0.950771</td>\n",
              "      <td>0.910641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.272600</td>\n",
              "      <td>0.381781</td>\n",
              "      <td>0.848536</td>\n",
              "      <td>0.854830</td>\n",
              "      <td>0.979799</td>\n",
              "      <td>0.913059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.192500</td>\n",
              "      <td>0.397547</td>\n",
              "      <td>0.848415</td>\n",
              "      <td>0.851871</td>\n",
              "      <td>0.984438</td>\n",
              "      <td>0.913369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.215300</td>\n",
              "      <td>0.389420</td>\n",
              "      <td>0.850237</td>\n",
              "      <td>0.860450</td>\n",
              "      <td>0.973365</td>\n",
              "      <td>0.913431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.214900</td>\n",
              "      <td>0.394368</td>\n",
              "      <td>0.851573</td>\n",
              "      <td>0.859987</td>\n",
              "      <td>0.976059</td>\n",
              "      <td>0.914354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.244900</td>\n",
              "      <td>0.367547</td>\n",
              "      <td>0.852423</td>\n",
              "      <td>0.863951</td>\n",
              "      <td>0.971121</td>\n",
              "      <td>0.914406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.233000</td>\n",
              "      <td>0.360845</td>\n",
              "      <td>0.850723</td>\n",
              "      <td>0.869713</td>\n",
              "      <td>0.959898</td>\n",
              "      <td>0.912583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.224200</td>\n",
              "      <td>0.373982</td>\n",
              "      <td>0.850358</td>\n",
              "      <td>0.869459</td>\n",
              "      <td>0.959749</td>\n",
              "      <td>0.912376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.255100</td>\n",
              "      <td>0.364427</td>\n",
              "      <td>0.852423</td>\n",
              "      <td>0.885505</td>\n",
              "      <td>0.939698</td>\n",
              "      <td>0.911797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.239400</td>\n",
              "      <td>0.371504</td>\n",
              "      <td>0.851694</td>\n",
              "      <td>0.871060</td>\n",
              "      <td>0.959300</td>\n",
              "      <td>0.913053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.261100</td>\n",
              "      <td>0.374396</td>\n",
              "      <td>0.849751</td>\n",
              "      <td>0.869170</td>\n",
              "      <td>0.959300</td>\n",
              "      <td>0.912014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.249000</td>\n",
              "      <td>0.368236</td>\n",
              "      <td>0.849387</td>\n",
              "      <td>0.883148</td>\n",
              "      <td>0.938650</td>\n",
              "      <td>0.910054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.221700</td>\n",
              "      <td>0.373194</td>\n",
              "      <td>0.849144</td>\n",
              "      <td>0.871805</td>\n",
              "      <td>0.954511</td>\n",
              "      <td>0.911286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.251700</td>\n",
              "      <td>0.375237</td>\n",
              "      <td>0.848172</td>\n",
              "      <td>0.863898</td>\n",
              "      <td>0.964986</td>\n",
              "      <td>0.911648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.219100</td>\n",
              "      <td>0.368449</td>\n",
              "      <td>0.850115</td>\n",
              "      <td>0.862542</td>\n",
              "      <td>0.969924</td>\n",
              "      <td>0.913086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.265200</td>\n",
              "      <td>0.354377</td>\n",
              "      <td>0.852302</td>\n",
              "      <td>0.881614</td>\n",
              "      <td>0.944935</td>\n",
              "      <td>0.912177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.227800</td>\n",
              "      <td>0.365402</td>\n",
              "      <td>0.849265</td>\n",
              "      <td>0.865726</td>\n",
              "      <td>0.963789</td>\n",
              "      <td>0.912129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.240700</td>\n",
              "      <td>0.379758</td>\n",
              "      <td>0.849508</td>\n",
              "      <td>0.861200</td>\n",
              "      <td>0.971121</td>\n",
              "      <td>0.912863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.214600</td>\n",
              "      <td>0.381052</td>\n",
              "      <td>0.850237</td>\n",
              "      <td>0.861694</td>\n",
              "      <td>0.971420</td>\n",
              "      <td>0.913273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.250800</td>\n",
              "      <td>0.372159</td>\n",
              "      <td>0.849994</td>\n",
              "      <td>0.877809</td>\n",
              "      <td>0.947030</td>\n",
              "      <td>0.911106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.242100</td>\n",
              "      <td>0.362106</td>\n",
              "      <td>0.849265</td>\n",
              "      <td>0.881948</td>\n",
              "      <td>0.940147</td>\n",
              "      <td>0.910118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.227600</td>\n",
              "      <td>0.373002</td>\n",
              "      <td>0.848658</td>\n",
              "      <td>0.871228</td>\n",
              "      <td>0.954661</td>\n",
              "      <td>0.911038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.218000</td>\n",
              "      <td>0.384611</td>\n",
              "      <td>0.849751</td>\n",
              "      <td>0.870678</td>\n",
              "      <td>0.957055</td>\n",
              "      <td>0.911826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.235000</td>\n",
              "      <td>0.380135</td>\n",
              "      <td>0.850237</td>\n",
              "      <td>0.869041</td>\n",
              "      <td>0.960198</td>\n",
              "      <td>0.912348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.247400</td>\n",
              "      <td>0.372212</td>\n",
              "      <td>0.848172</td>\n",
              "      <td>0.862055</td>\n",
              "      <td>0.967829</td>\n",
              "      <td>0.911885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.213100</td>\n",
              "      <td>0.383522</td>\n",
              "      <td>0.846350</td>\n",
              "      <td>0.859570</td>\n",
              "      <td>0.969026</td>\n",
              "      <td>0.911022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.231600</td>\n",
              "      <td>0.381530</td>\n",
              "      <td>0.846472</td>\n",
              "      <td>0.861026</td>\n",
              "      <td>0.966931</td>\n",
              "      <td>0.910911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.250100</td>\n",
              "      <td>0.375152</td>\n",
              "      <td>0.845257</td>\n",
              "      <td>0.862874</td>\n",
              "      <td>0.962292</td>\n",
              "      <td>0.909875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.207700</td>\n",
              "      <td>0.370366</td>\n",
              "      <td>0.847443</td>\n",
              "      <td>0.874224</td>\n",
              "      <td>0.948526</td>\n",
              "      <td>0.909861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.237200</td>\n",
              "      <td>0.373046</td>\n",
              "      <td>0.845986</td>\n",
              "      <td>0.870535</td>\n",
              "      <td>0.951818</td>\n",
              "      <td>0.909364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.282700</td>\n",
              "      <td>0.367472</td>\n",
              "      <td>0.846836</td>\n",
              "      <td>0.868844</td>\n",
              "      <td>0.955559</td>\n",
              "      <td>0.910140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.241000</td>\n",
              "      <td>0.377375</td>\n",
              "      <td>0.846957</td>\n",
              "      <td>0.861100</td>\n",
              "      <td>0.967530</td>\n",
              "      <td>0.911218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.223800</td>\n",
              "      <td>0.373758</td>\n",
              "      <td>0.847686</td>\n",
              "      <td>0.861307</td>\n",
              "      <td>0.968278</td>\n",
              "      <td>0.911665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.252900</td>\n",
              "      <td>0.367887</td>\n",
              "      <td>0.846593</td>\n",
              "      <td>0.867507</td>\n",
              "      <td>0.957205</td>\n",
              "      <td>0.910152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>0.211400</td>\n",
              "      <td>0.369171</td>\n",
              "      <td>0.849387</td>\n",
              "      <td>0.867125</td>\n",
              "      <td>0.961843</td>\n",
              "      <td>0.912032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.234200</td>\n",
              "      <td>0.373722</td>\n",
              "      <td>0.850115</td>\n",
              "      <td>0.867927</td>\n",
              "      <td>0.961694</td>\n",
              "      <td>0.912408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.215700</td>\n",
              "      <td>0.376682</td>\n",
              "      <td>0.847929</td>\n",
              "      <td>0.870514</td>\n",
              "      <td>0.954661</td>\n",
              "      <td>0.910648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.239400</td>\n",
              "      <td>0.382427</td>\n",
              "      <td>0.849144</td>\n",
              "      <td>0.870590</td>\n",
              "      <td>0.956307</td>\n",
              "      <td>0.911438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.250600</td>\n",
              "      <td>0.379427</td>\n",
              "      <td>0.848293</td>\n",
              "      <td>0.871581</td>\n",
              "      <td>0.953614</td>\n",
              "      <td>0.910754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.255700</td>\n",
              "      <td>0.374028</td>\n",
              "      <td>0.850358</td>\n",
              "      <td>0.874022</td>\n",
              "      <td>0.953015</td>\n",
              "      <td>0.911811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.234100</td>\n",
              "      <td>0.367529</td>\n",
              "      <td>0.848293</td>\n",
              "      <td>0.878412</td>\n",
              "      <td>0.943738</td>\n",
              "      <td>0.909904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.227000</td>\n",
              "      <td>0.368347</td>\n",
              "      <td>0.851209</td>\n",
              "      <td>0.877664</td>\n",
              "      <td>0.948975</td>\n",
              "      <td>0.911928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.251000</td>\n",
              "      <td>0.367054</td>\n",
              "      <td>0.851209</td>\n",
              "      <td>0.877664</td>\n",
              "      <td>0.948975</td>\n",
              "      <td>0.911928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.256200</td>\n",
              "      <td>0.366393</td>\n",
              "      <td>0.850115</td>\n",
              "      <td>0.876781</td>\n",
              "      <td>0.948676</td>\n",
              "      <td>0.911312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.246600</td>\n",
              "      <td>0.367101</td>\n",
              "      <td>0.849872</td>\n",
              "      <td>0.870595</td>\n",
              "      <td>0.957354</td>\n",
              "      <td>0.911916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.256100</td>\n",
              "      <td>0.364866</td>\n",
              "      <td>0.850966</td>\n",
              "      <td>0.869848</td>\n",
              "      <td>0.960048</td>\n",
              "      <td>0.912725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.365732</td>\n",
              "      <td>0.852302</td>\n",
              "      <td>0.874093</td>\n",
              "      <td>0.955709</td>\n",
              "      <td>0.913081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>0.233600</td>\n",
              "      <td>0.369424</td>\n",
              "      <td>0.852302</td>\n",
              "      <td>0.874093</td>\n",
              "      <td>0.955709</td>\n",
              "      <td>0.913081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>0.207000</td>\n",
              "      <td>0.375530</td>\n",
              "      <td>0.851087</td>\n",
              "      <td>0.869866</td>\n",
              "      <td>0.960198</td>\n",
              "      <td>0.912802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>0.250700</td>\n",
              "      <td>0.377797</td>\n",
              "      <td>0.851087</td>\n",
              "      <td>0.869866</td>\n",
              "      <td>0.960198</td>\n",
              "      <td>0.912802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>0.236200</td>\n",
              "      <td>0.375503</td>\n",
              "      <td>0.851087</td>\n",
              "      <td>0.869866</td>\n",
              "      <td>0.960198</td>\n",
              "      <td>0.912802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.226600</td>\n",
              "      <td>0.374109</td>\n",
              "      <td>0.851937</td>\n",
              "      <td>0.870692</td>\n",
              "      <td>0.960198</td>\n",
              "      <td>0.913257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>0.244900</td>\n",
              "      <td>0.373176</td>\n",
              "      <td>0.851937</td>\n",
              "      <td>0.870692</td>\n",
              "      <td>0.960198</td>\n",
              "      <td>0.913257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>0.188700</td>\n",
              "      <td>0.373472</td>\n",
              "      <td>0.851937</td>\n",
              "      <td>0.870692</td>\n",
              "      <td>0.960198</td>\n",
              "      <td>0.913257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>0.218500</td>\n",
              "      <td>0.373589</td>\n",
              "      <td>0.851937</td>\n",
              "      <td>0.870692</td>\n",
              "      <td>0.960198</td>\n",
              "      <td>0.913257</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [129/129 00:20]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 results: {'eval_loss': 0.35242268443107605, 'eval_accuracy': 0.8569172841005709, 'eval_precision': 0.885990744636096, 'eval_recall': 0.9453838096663175, 'eval_f1': 0.9147241928478356, 'eval_runtime': 20.5292, 'eval_samples_per_second': 401.039, 'eval_steps_per_second': 6.284, 'epoch': 3.0}\n",
            "Fold 2 results: {'eval_loss': 0.35242268443107605, 'eval_accuracy': 0.8569172841005709, 'eval_precision': 0.885990744636096, 'eval_recall': 0.9453838096663175, 'eval_f1': 0.9147241928478356, 'eval_runtime': 20.5292, 'eval_samples_per_second': 401.039, 'eval_steps_per_second': 6.284, 'epoch': 3.0}\n",
            "Training on fold 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  51/1350 01:30 < 39:55, 0.54 it/s, Epoch 0.11/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.304100</td>\n",
              "      <td>0.269398</td>\n",
              "      <td>0.879255</td>\n",
              "      <td>0.882655</td>\n",
              "      <td>0.980541</td>\n",
              "      <td>0.929026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.308800</td>\n",
              "      <td>0.259524</td>\n",
              "      <td>0.877598</td>\n",
              "      <td>0.894714</td>\n",
              "      <td>0.961240</td>\n",
              "      <td>0.926785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.258200</td>\n",
              "      <td>0.265112</td>\n",
              "      <td>0.878235</td>\n",
              "      <td>0.880244</td>\n",
              "      <td>0.982598</td>\n",
              "      <td>0.928609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.289800</td>\n",
              "      <td>0.256336</td>\n",
              "      <td>0.880530</td>\n",
              "      <td>0.917364</td>\n",
              "      <td>0.936086</td>\n",
              "      <td>0.926631</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='71' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 71/123 00:11 < 00:08, 6.32 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9b29b65d5de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Place 5 for all fold results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training on fold {fold_idx}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fold {fold_idx} results:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-06150c7a47e1>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(fold_idx, tokenizer, model, max_len, batch_size, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Evaluate the model on test fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1933\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2343\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2791\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2793\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2750\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3640\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3641\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3642\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3643\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3825\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3826\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3827\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4038\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4039\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4040\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4041\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_results)\n",
        "#print(\"hello\")"
      ],
      "metadata": {
        "id": "j9png8ajxzyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Write Train Data to text File"
      ],
      "metadata": {
        "id": "i2C3ACvuPxDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def prepare_textFILE(fold_idx):\n",
        "\n",
        "    true_positive=[]\n",
        "    false_positive=[]\n",
        "\n",
        "    filename=[]\n",
        "    for i in range(5):\n",
        "          if i==fold_idx:\n",
        "              continue\n",
        "          name = \"data/final_data_fold_\"+str(i)+\".csv\"\n",
        "          filename.append(name)\n",
        "    #print(filename)\n",
        "    df1 = pd.read_csv(filename[0])\n",
        "    df2 = pd.read_csv(filename[1])\n",
        "    df3 = pd.read_csv(filename[2])\n",
        "    df4 = pd.read_csv(filename[3])\n",
        "\n",
        "    merged_df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
        "\n",
        "    # Filter rows where 'LABEL' is 1 (true positive) and 0 (false positive)\n",
        "    true_positive = merged_df[merged_df['LABEL'] == 1]['response']\n",
        "    false_positive = merged_df[merged_df['LABEL'] == 0]['response']\n",
        "\n",
        "\n",
        "\n",
        "    # Write true positive responses to a file\n",
        "    with open(f'bc_data/processed_data_true_positive_train_{fold_idx}.txt', 'w') as f_true:\n",
        "        for response in true_positive:\n",
        "            f_true.write(f\"{response}\\n\")\n",
        "\n",
        "    # Write false positive responses to a file\n",
        "    with open(f'bc_data/processed_data_false_positive_train_{fold_idx}.txt', 'w') as f_false:\n",
        "        for response in false_positive:\n",
        "            f_false.write(f\"{response}\\n\")\n",
        "\n",
        "    print(\"Files created successfully!\")\n"
      ],
      "metadata": {
        "id": "o5dj5vjpPzDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  prepare_textFILE(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGPWU3yURBZn",
        "outputId": "1fe7cb57-388e-44e8-df3c-7620ea878bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files created successfully!\n",
            "Files created successfully!\n",
            "Files created successfully!\n",
            "Files created successfully!\n",
            "Files created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Write Test Data to text File"
      ],
      "metadata": {
        "id": "_5xDx6m9SG5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def prepare_textFILE_Test(fold_idx):\n",
        "\n",
        "    true_positive=[]\n",
        "    false_positive=[]\n",
        "\n",
        "\n",
        "\n",
        "    df = pd.read_csv(f'data/final_data_fold_{fold_idx}.csv')\n",
        "\n",
        "    # Filter rows where 'LABEL' is 1 (true positive) and 0 (false positive)\n",
        "    true_positive = df[df['LABEL'] == 1]['response']\n",
        "    false_positive = df[df['LABEL'] == 0]['response']\n",
        "\n",
        "\n",
        "    # Write true positive responses to a file\n",
        "    with open(f'bc_data/processed_data_true_positive_test_{fold_idx}.txt', 'w') as f_true:\n",
        "        for response in true_positive:\n",
        "            f_true.write(f\"{response}\\n\")\n",
        "\n",
        "    # Write false positive responses to a file\n",
        "    with open(f'bc_data/processed_data_false_positive_test_{fold_idx}.txt', 'w') as f_false:\n",
        "        for response in false_positive:\n",
        "            f_false.write(f\"{response}\\n\")\n",
        "\n",
        "    print(\"Files created successfully!\")\n"
      ],
      "metadata": {
        "id": "8GHulRY2SKF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  prepare_textFILE_Test(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsmV3XeTSqjL",
        "outputId": "8dcff31f-d0fb-46ce-f5c8-852d287dd957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files created successfully!\n",
            "Files created successfully!\n",
            "Files created successfully!\n",
            "Files created successfully!\n",
            "Files created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary Classifier - Distilbert"
      ],
      "metadata": {
        "id": "GIvfdQM-rNun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code is adapted from:\n",
        "# 1. HuggingFace tutorial on using DistillBert https://huggingface.co/distilbert/distilbert-base-uncased\n",
        "# 2. Huggingface tutorial on training transformers for sequence classification here: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
        "\n",
        "### Importing libraries\n",
        "import argparse\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler, SequentialSampler\n",
        "\n",
        "# specify the available devices\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "## Function for reading the given file\n",
        "def read_text(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    lines = [l.strip() for l in lines]\n",
        "  return pd.DataFrame(lines)\n",
        "\n",
        "# Set seed\n",
        "seed = 912\n",
        "\n",
        "## Parser for setting input values\n",
        "parser = argparse.ArgumentParser(description='Adversarial masks for the safety classifier.')\n",
        "parser.add_argument('--safe_train', type=str, default='data/safe_prompts_train_insertion_erased.txt', help='File containing safe prompts for training')\n",
        "parser.add_argument('--harmful_train', type=str, default='data/harmful_prompts_train.txt', help='File containing harmful prompts for training')\n",
        "parser.add_argument('--safe_test', type=str, default='data/safe_prompts_test_insertion_erased.txt', help='File containing safe prompts for testing')\n",
        "parser.add_argument('--harmful_test', type=str, default='data/harmful_prompts_test.txt', help='File containing harmful prompts for testing')\n",
        "parser.add_argument('--save_path', type=str, default='models/distilbert_insertion.pt', help='Path to save the model')\n",
        "\n",
        "args = parser.parse_args(['--safe_train', 'bc_data/processed_data_true_positive_train_0.txt',\n",
        "                          '--harmful_train', 'bc_data/processed_data_false_positive_train_0.txt',\n",
        "                          '--safe_test', 'bc_data/processed_data_true_positive_test_0.txt',\n",
        "                          '--harmful_test', 'bc_data/processed_data_false_positive_test_0.txt',\n",
        "                          '--save_path', 'bc_data/distilbert_insertion.pt'\n",
        "                          ])\n",
        "\n",
        "# Load safe and harmful prompts and create the dataset for training classifier\n",
        "# Class 1: Safe, Class 0: Harmful\n",
        "safe_prompt_train = read_text(args.safe_train)\n",
        "harm_prompt_train = read_text(args.harmful_train)\n",
        "prompt_data_train = pd.concat([safe_prompt_train, harm_prompt_train], ignore_index=True)\n",
        "prompt_data_train['Y'] = pd.Series(np.concatenate([np.ones(safe_prompt_train.shape[0]), np.zeros(harm_prompt_train.shape[0])])).astype(int)\n",
        "\n",
        "# Split train dataset into train and validation sets\n",
        "train_text, val_text, train_labels, val_labels = train_test_split(prompt_data_train[0],\n",
        "\t\t\t\t\t\t\t\tprompt_data_train['Y'],\n",
        "\t\t\t\t\t\t\t\trandom_state=seed,\n",
        "\t\t\t\t\t\t\t\ttest_size=0.2,\n",
        "\t\t\t\t\t\t\t\tstratify=prompt_data_train['Y'])\n",
        "\n",
        "# Count number of samples in each class in the training set\n",
        "count = train_labels.value_counts().to_dict()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# pass the pre-trained DistilBert to our define architecture\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "# print(model)\n",
        "\n",
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "## Convert lists to tensors for train split\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "sample_weights = torch.tensor([1/count[i] for i in train_labels])\n",
        "\n",
        "## Convert lists to tensors for validation split\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "# train_sampler = RandomSampler(train_data)\n",
        "train_sampler = WeightedRandomSampler(sample_weights, len(train_data), replacement=True)\n",
        "\n",
        "# dataLoader for the train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-5)          # learning rate\n",
        "\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# #compute the class weights\n",
        "# class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_labels), y = train_labels.to_numpy())\n",
        "\n",
        "# print(\"Class Weights:\",class_weights)\n",
        "\n",
        "# # converting list of class weights to a tensor\n",
        "# weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# # push to GPU\n",
        "# weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "# loss_fn  = nn.NLLLoss(weight=weights)\n",
        "# loss_fn  = nn.CrossEntropyLoss(weight=weights)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# loss_fn = nn.NLLLoss()\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 3\n",
        "\n",
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "\n",
        "  # iterate over batches\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    # progress update after every 50 batches.\n",
        "    if (step + 1) % 50 == 0 or step == len(train_dataloader) - 1:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step + 1, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)[0]\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = loss_fn(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds\n",
        "\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "\n",
        "  print(\"\\nEvaluating...\")\n",
        "\n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "\n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "\n",
        "    # Progress update every 50 batches.\n",
        "    if (step + 1) % 50 == 0 or step == len(val_dataloader) - 1:\n",
        "\n",
        "      # Calculate elapsed time in minutes.\n",
        "      # elapsed = format_time(time.time() - t0)\n",
        "\n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step + 1, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)[0]\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = loss_fn(preds, labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds\n",
        "\n",
        "# set initial loss to infinite\n",
        "best_validation_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "training_losses=[]\n",
        "validation_losses=[]\n",
        "train_flag = True\n",
        "\n",
        "if train_flag == True:\n",
        "    # for each epoch\n",
        "    for epoch in range(epochs):\n",
        "        # Copilot Code Reference: Similar code with 2 license types [MIT, unknown]\n",
        "        # https://github.com/github-copilot/code_referencing?cursor=ca31ec3ebd8e24ea9127b39656a9ec6b&editor=vscode\n",
        "        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "        #train model\n",
        "        training_loss, _ = train()\n",
        "\n",
        "        #evaluate model\n",
        "        validation_loss, _ = evaluate()\n",
        "\n",
        "        #save the best model\n",
        "        if validation_loss < best_validation_loss:\n",
        "            best_validation_loss = validation_loss\n",
        "            torch.save(model.state_dict(), args.save_path)\n",
        "            # torch.save(model.state_dict(), 'new_distillbert_saved_weights.pt')\n",
        "\n",
        "        # append training and validation loss\n",
        "        training_losses.append(training_loss)\n",
        "        validation_losses.append(validation_loss)\n",
        "\n",
        "        print(f'\\nTraining Loss: {training_loss:.3f}')\n",
        "        print(f'Validation Loss: {validation_loss:.3f}')\n",
        "\n",
        "\n",
        "# Test safety classifier\n",
        "safe_prompt_test = read_text(args.safe_test)\n",
        "harm_prompt_test = read_text(args.harmful_test)\n",
        "prompt_data_test = pd.concat([safe_prompt_test, harm_prompt_test], ignore_index=True)\n",
        "prompt_data_test['Y'] = pd.Series(np.concatenate([np.ones(safe_prompt_test.shape[0]), np.zeros(harm_prompt_test.shape[0])])).astype(int)\n",
        "\n",
        "test_text = prompt_data_test[0]\n",
        "test_labels = prompt_data_test['Y']\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "#load weights of best model\n",
        "# path = args.save_path\n",
        "# # path = 'new_distillbert_saved_weights.pt'\n",
        "# model.load_state_dict(torch.load(path))\n",
        "# model.eval()\n",
        "\n",
        "# # get predictions for test data\n",
        "# with torch.no_grad():\n",
        "#   preds = model(test_seq.to(device), test_mask.to(device))[0]\n",
        "#   preds = preds.detach().cpu().numpy()\n",
        "\n",
        "# preds = np.argmax(preds, axis = 1)\n",
        "# print(f'Testing Accuracy = {100*torch.sum(torch.tensor(preds) == test_y)/test_y.shape[0]}%')\n",
        "# print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hlFQJq8rRJA",
        "outputId": "414dee6b-3368-4942-92fc-fe9c2fbc9809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 3\n",
            "  Batch    50  of  6,362.\n",
            "  Batch   100  of  6,362.\n",
            "  Batch   150  of  6,362.\n",
            "  Batch   200  of  6,362.\n",
            "  Batch   250  of  6,362.\n",
            "  Batch   300  of  6,362.\n",
            "  Batch   350  of  6,362.\n",
            "  Batch   400  of  6,362.\n",
            "  Batch   450  of  6,362.\n",
            "  Batch   500  of  6,362.\n",
            "  Batch   550  of  6,362.\n",
            "  Batch   600  of  6,362.\n",
            "  Batch   650  of  6,362.\n",
            "  Batch   700  of  6,362.\n",
            "  Batch   750  of  6,362.\n",
            "  Batch   800  of  6,362.\n",
            "  Batch   850  of  6,362.\n",
            "  Batch   900  of  6,362.\n",
            "  Batch   950  of  6,362.\n",
            "  Batch 1,000  of  6,362.\n",
            "  Batch 1,050  of  6,362.\n",
            "  Batch 1,100  of  6,362.\n",
            "  Batch 1,150  of  6,362.\n",
            "  Batch 1,200  of  6,362.\n",
            "  Batch 1,250  of  6,362.\n",
            "  Batch 1,300  of  6,362.\n",
            "  Batch 1,350  of  6,362.\n",
            "  Batch 1,400  of  6,362.\n",
            "  Batch 1,450  of  6,362.\n",
            "  Batch 1,500  of  6,362.\n",
            "  Batch 1,550  of  6,362.\n",
            "  Batch 1,600  of  6,362.\n",
            "  Batch 1,650  of  6,362.\n",
            "  Batch 1,700  of  6,362.\n",
            "  Batch 1,750  of  6,362.\n",
            "  Batch 1,800  of  6,362.\n",
            "  Batch 1,850  of  6,362.\n",
            "  Batch 1,900  of  6,362.\n",
            "  Batch 1,950  of  6,362.\n",
            "  Batch 2,000  of  6,362.\n",
            "  Batch 2,050  of  6,362.\n",
            "  Batch 2,100  of  6,362.\n",
            "  Batch 2,150  of  6,362.\n",
            "  Batch 2,200  of  6,362.\n",
            "  Batch 2,250  of  6,362.\n",
            "  Batch 2,300  of  6,362.\n",
            "  Batch 2,350  of  6,362.\n",
            "  Batch 2,400  of  6,362.\n",
            "  Batch 2,450  of  6,362.\n",
            "  Batch 2,500  of  6,362.\n",
            "  Batch 2,550  of  6,362.\n",
            "  Batch 2,600  of  6,362.\n",
            "  Batch 2,650  of  6,362.\n",
            "  Batch 2,700  of  6,362.\n",
            "  Batch 2,750  of  6,362.\n",
            "  Batch 2,800  of  6,362.\n",
            "  Batch 2,850  of  6,362.\n",
            "  Batch 2,900  of  6,362.\n",
            "  Batch 2,950  of  6,362.\n",
            "  Batch 3,000  of  6,362.\n",
            "  Batch 3,050  of  6,362.\n",
            "  Batch 3,100  of  6,362.\n",
            "  Batch 3,150  of  6,362.\n",
            "  Batch 3,200  of  6,362.\n",
            "  Batch 3,250  of  6,362.\n",
            "  Batch 3,300  of  6,362.\n",
            "  Batch 3,350  of  6,362.\n",
            "  Batch 3,400  of  6,362.\n",
            "  Batch 3,450  of  6,362.\n",
            "  Batch 3,500  of  6,362.\n",
            "  Batch 3,550  of  6,362.\n",
            "  Batch 3,600  of  6,362.\n",
            "  Batch 3,650  of  6,362.\n",
            "  Batch 3,700  of  6,362.\n",
            "  Batch 3,750  of  6,362.\n",
            "  Batch 3,800  of  6,362.\n",
            "  Batch 3,850  of  6,362.\n",
            "  Batch 3,900  of  6,362.\n",
            "  Batch 3,950  of  6,362.\n",
            "  Batch 4,000  of  6,362.\n",
            "  Batch 4,050  of  6,362.\n",
            "  Batch 4,100  of  6,362.\n",
            "  Batch 4,150  of  6,362.\n",
            "  Batch 4,200  of  6,362.\n",
            "  Batch 4,250  of  6,362.\n",
            "  Batch 4,300  of  6,362.\n",
            "  Batch 4,350  of  6,362.\n",
            "  Batch 4,400  of  6,362.\n",
            "  Batch 4,450  of  6,362.\n",
            "  Batch 4,500  of  6,362.\n",
            "  Batch 4,550  of  6,362.\n",
            "  Batch 4,600  of  6,362.\n",
            "  Batch 4,650  of  6,362.\n",
            "  Batch 4,700  of  6,362.\n",
            "  Batch 4,750  of  6,362.\n",
            "  Batch 4,800  of  6,362.\n",
            "  Batch 4,850  of  6,362.\n",
            "  Batch 4,900  of  6,362.\n",
            "  Batch 4,950  of  6,362.\n",
            "  Batch 5,000  of  6,362.\n",
            "  Batch 5,050  of  6,362.\n",
            "  Batch 5,100  of  6,362.\n",
            "  Batch 5,150  of  6,362.\n",
            "  Batch 5,200  of  6,362.\n",
            "  Batch 5,250  of  6,362.\n",
            "  Batch 5,300  of  6,362.\n",
            "  Batch 5,350  of  6,362.\n",
            "  Batch 5,400  of  6,362.\n",
            "  Batch 5,450  of  6,362.\n",
            "  Batch 5,500  of  6,362.\n",
            "  Batch 5,550  of  6,362.\n",
            "  Batch 5,600  of  6,362.\n",
            "  Batch 5,650  of  6,362.\n",
            "  Batch 5,700  of  6,362.\n",
            "  Batch 5,750  of  6,362.\n",
            "  Batch 5,800  of  6,362.\n",
            "  Batch 5,850  of  6,362.\n",
            "  Batch 5,900  of  6,362.\n",
            "  Batch 5,950  of  6,362.\n",
            "  Batch 6,000  of  6,362.\n",
            "  Batch 6,050  of  6,362.\n",
            "  Batch 6,100  of  6,362.\n",
            "  Batch 6,150  of  6,362.\n",
            "  Batch 6,200  of  6,362.\n",
            "  Batch 6,250  of  6,362.\n",
            "  Batch 6,300  of  6,362.\n",
            "  Batch 6,350  of  6,362.\n",
            "  Batch 6,362  of  6,362.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,591.\n",
            "  Batch   100  of  1,591.\n",
            "  Batch   150  of  1,591.\n",
            "  Batch   200  of  1,591.\n",
            "  Batch   250  of  1,591.\n",
            "  Batch   300  of  1,591.\n",
            "  Batch   350  of  1,591.\n",
            "  Batch   400  of  1,591.\n",
            "  Batch   450  of  1,591.\n",
            "  Batch   500  of  1,591.\n",
            "  Batch   550  of  1,591.\n",
            "  Batch   600  of  1,591.\n",
            "  Batch   650  of  1,591.\n",
            "  Batch   700  of  1,591.\n",
            "  Batch   750  of  1,591.\n",
            "  Batch   800  of  1,591.\n",
            "  Batch   850  of  1,591.\n",
            "  Batch   900  of  1,591.\n",
            "  Batch   950  of  1,591.\n",
            "  Batch 1,000  of  1,591.\n",
            "  Batch 1,050  of  1,591.\n",
            "  Batch 1,100  of  1,591.\n",
            "  Batch 1,150  of  1,591.\n",
            "  Batch 1,200  of  1,591.\n",
            "  Batch 1,250  of  1,591.\n",
            "  Batch 1,300  of  1,591.\n",
            "  Batch 1,350  of  1,591.\n",
            "  Batch 1,400  of  1,591.\n",
            "  Batch 1,450  of  1,591.\n",
            "  Batch 1,500  of  1,591.\n",
            "  Batch 1,550  of  1,591.\n",
            "  Batch 1,591  of  1,591.\n",
            "\n",
            "Training Loss: 0.528\n",
            "Validation Loss: 0.466\n",
            "\n",
            " Epoch 2 / 3\n",
            "  Batch    50  of  6,362.\n",
            "  Batch   100  of  6,362.\n",
            "  Batch   150  of  6,362.\n",
            "  Batch   200  of  6,362.\n",
            "  Batch   250  of  6,362.\n",
            "  Batch   300  of  6,362.\n",
            "  Batch   350  of  6,362.\n",
            "  Batch   400  of  6,362.\n",
            "  Batch   450  of  6,362.\n",
            "  Batch   500  of  6,362.\n",
            "  Batch   550  of  6,362.\n",
            "  Batch   600  of  6,362.\n",
            "  Batch   650  of  6,362.\n",
            "  Batch   700  of  6,362.\n",
            "  Batch   750  of  6,362.\n",
            "  Batch   800  of  6,362.\n",
            "  Batch   850  of  6,362.\n",
            "  Batch   900  of  6,362.\n",
            "  Batch   950  of  6,362.\n",
            "  Batch 1,000  of  6,362.\n",
            "  Batch 1,050  of  6,362.\n",
            "  Batch 1,100  of  6,362.\n",
            "  Batch 1,150  of  6,362.\n",
            "  Batch 1,200  of  6,362.\n",
            "  Batch 1,250  of  6,362.\n",
            "  Batch 1,300  of  6,362.\n",
            "  Batch 1,350  of  6,362.\n",
            "  Batch 1,400  of  6,362.\n",
            "  Batch 1,450  of  6,362.\n",
            "  Batch 1,500  of  6,362.\n",
            "  Batch 1,550  of  6,362.\n",
            "  Batch 1,600  of  6,362.\n",
            "  Batch 1,650  of  6,362.\n",
            "  Batch 1,700  of  6,362.\n",
            "  Batch 1,750  of  6,362.\n",
            "  Batch 1,800  of  6,362.\n",
            "  Batch 1,850  of  6,362.\n",
            "  Batch 1,900  of  6,362.\n",
            "  Batch 1,950  of  6,362.\n",
            "  Batch 2,000  of  6,362.\n",
            "  Batch 2,050  of  6,362.\n",
            "  Batch 2,100  of  6,362.\n",
            "  Batch 2,150  of  6,362.\n",
            "  Batch 2,200  of  6,362.\n",
            "  Batch 2,250  of  6,362.\n",
            "  Batch 2,300  of  6,362.\n",
            "  Batch 2,350  of  6,362.\n",
            "  Batch 2,400  of  6,362.\n",
            "  Batch 2,450  of  6,362.\n",
            "  Batch 2,500  of  6,362.\n",
            "  Batch 2,550  of  6,362.\n",
            "  Batch 2,600  of  6,362.\n",
            "  Batch 2,650  of  6,362.\n",
            "  Batch 2,700  of  6,362.\n",
            "  Batch 2,750  of  6,362.\n",
            "  Batch 2,800  of  6,362.\n",
            "  Batch 2,850  of  6,362.\n",
            "  Batch 2,900  of  6,362.\n",
            "  Batch 2,950  of  6,362.\n",
            "  Batch 3,000  of  6,362.\n",
            "  Batch 3,050  of  6,362.\n",
            "  Batch 3,100  of  6,362.\n",
            "  Batch 3,150  of  6,362.\n",
            "  Batch 3,200  of  6,362.\n",
            "  Batch 3,250  of  6,362.\n",
            "  Batch 3,300  of  6,362.\n",
            "  Batch 3,350  of  6,362.\n",
            "  Batch 3,400  of  6,362.\n",
            "  Batch 3,450  of  6,362.\n",
            "  Batch 3,500  of  6,362.\n",
            "  Batch 3,550  of  6,362.\n",
            "  Batch 3,600  of  6,362.\n",
            "  Batch 3,650  of  6,362.\n",
            "  Batch 3,700  of  6,362.\n",
            "  Batch 3,750  of  6,362.\n",
            "  Batch 3,800  of  6,362.\n",
            "  Batch 3,850  of  6,362.\n",
            "  Batch 3,900  of  6,362.\n",
            "  Batch 3,950  of  6,362.\n",
            "  Batch 4,000  of  6,362.\n",
            "  Batch 4,050  of  6,362.\n",
            "  Batch 4,100  of  6,362.\n",
            "  Batch 4,150  of  6,362.\n",
            "  Batch 4,200  of  6,362.\n",
            "  Batch 4,250  of  6,362.\n",
            "  Batch 4,300  of  6,362.\n",
            "  Batch 4,350  of  6,362.\n",
            "  Batch 4,400  of  6,362.\n",
            "  Batch 4,450  of  6,362.\n",
            "  Batch 4,500  of  6,362.\n",
            "  Batch 4,550  of  6,362.\n",
            "  Batch 4,600  of  6,362.\n",
            "  Batch 4,650  of  6,362.\n",
            "  Batch 4,700  of  6,362.\n",
            "  Batch 4,750  of  6,362.\n",
            "  Batch 4,800  of  6,362.\n",
            "  Batch 4,850  of  6,362.\n",
            "  Batch 4,900  of  6,362.\n",
            "  Batch 4,950  of  6,362.\n",
            "  Batch 5,000  of  6,362.\n",
            "  Batch 5,050  of  6,362.\n",
            "  Batch 5,100  of  6,362.\n",
            "  Batch 5,150  of  6,362.\n",
            "  Batch 5,200  of  6,362.\n",
            "  Batch 5,250  of  6,362.\n",
            "  Batch 5,300  of  6,362.\n",
            "  Batch 5,350  of  6,362.\n",
            "  Batch 5,400  of  6,362.\n",
            "  Batch 5,450  of  6,362.\n",
            "  Batch 5,500  of  6,362.\n",
            "  Batch 5,550  of  6,362.\n",
            "  Batch 5,600  of  6,362.\n",
            "  Batch 5,650  of  6,362.\n",
            "  Batch 5,700  of  6,362.\n",
            "  Batch 5,750  of  6,362.\n",
            "  Batch 5,800  of  6,362.\n",
            "  Batch 5,850  of  6,362.\n",
            "  Batch 5,900  of  6,362.\n",
            "  Batch 5,950  of  6,362.\n",
            "  Batch 6,000  of  6,362.\n",
            "  Batch 6,050  of  6,362.\n",
            "  Batch 6,100  of  6,362.\n",
            "  Batch 6,150  of  6,362.\n",
            "  Batch 6,200  of  6,362.\n",
            "  Batch 6,250  of  6,362.\n",
            "  Batch 6,300  of  6,362.\n",
            "  Batch 6,350  of  6,362.\n",
            "  Batch 6,362  of  6,362.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,591.\n",
            "  Batch   100  of  1,591.\n",
            "  Batch   150  of  1,591.\n",
            "  Batch   200  of  1,591.\n",
            "  Batch   250  of  1,591.\n",
            "  Batch   300  of  1,591.\n",
            "  Batch   350  of  1,591.\n",
            "  Batch   400  of  1,591.\n",
            "  Batch   450  of  1,591.\n",
            "  Batch   500  of  1,591.\n",
            "  Batch   550  of  1,591.\n",
            "  Batch   600  of  1,591.\n",
            "  Batch   650  of  1,591.\n",
            "  Batch   700  of  1,591.\n",
            "  Batch   750  of  1,591.\n",
            "  Batch   800  of  1,591.\n",
            "  Batch   850  of  1,591.\n",
            "  Batch   900  of  1,591.\n",
            "  Batch   950  of  1,591.\n",
            "  Batch 1,000  of  1,591.\n",
            "  Batch 1,050  of  1,591.\n",
            "  Batch 1,100  of  1,591.\n",
            "  Batch 1,150  of  1,591.\n",
            "  Batch 1,200  of  1,591.\n",
            "  Batch 1,250  of  1,591.\n",
            "  Batch 1,300  of  1,591.\n",
            "  Batch 1,350  of  1,591.\n",
            "  Batch 1,400  of  1,591.\n",
            "  Batch 1,450  of  1,591.\n",
            "  Batch 1,500  of  1,591.\n",
            "  Batch 1,550  of  1,591.\n",
            "  Batch 1,591  of  1,591.\n",
            "\n",
            "Training Loss: 0.483\n",
            "Validation Loss: 0.482\n",
            "\n",
            " Epoch 3 / 3\n",
            "  Batch    50  of  6,362.\n",
            "  Batch   100  of  6,362.\n",
            "  Batch   150  of  6,362.\n",
            "  Batch   200  of  6,362.\n",
            "  Batch   250  of  6,362.\n",
            "  Batch   300  of  6,362.\n",
            "  Batch   350  of  6,362.\n",
            "  Batch   400  of  6,362.\n",
            "  Batch   450  of  6,362.\n",
            "  Batch   500  of  6,362.\n",
            "  Batch   550  of  6,362.\n",
            "  Batch   600  of  6,362.\n",
            "  Batch   650  of  6,362.\n",
            "  Batch   700  of  6,362.\n",
            "  Batch   750  of  6,362.\n",
            "  Batch   800  of  6,362.\n",
            "  Batch   850  of  6,362.\n",
            "  Batch   900  of  6,362.\n",
            "  Batch   950  of  6,362.\n",
            "  Batch 1,000  of  6,362.\n",
            "  Batch 1,050  of  6,362.\n",
            "  Batch 1,100  of  6,362.\n",
            "  Batch 1,150  of  6,362.\n",
            "  Batch 1,200  of  6,362.\n",
            "  Batch 1,250  of  6,362.\n",
            "  Batch 1,300  of  6,362.\n",
            "  Batch 1,350  of  6,362.\n",
            "  Batch 1,400  of  6,362.\n",
            "  Batch 1,450  of  6,362.\n",
            "  Batch 1,500  of  6,362.\n",
            "  Batch 1,550  of  6,362.\n",
            "  Batch 1,600  of  6,362.\n",
            "  Batch 1,650  of  6,362.\n",
            "  Batch 1,700  of  6,362.\n",
            "  Batch 1,750  of  6,362.\n",
            "  Batch 1,800  of  6,362.\n",
            "  Batch 1,850  of  6,362.\n",
            "  Batch 1,900  of  6,362.\n",
            "  Batch 1,950  of  6,362.\n",
            "  Batch 2,000  of  6,362.\n",
            "  Batch 2,050  of  6,362.\n",
            "  Batch 2,100  of  6,362.\n",
            "  Batch 2,150  of  6,362.\n",
            "  Batch 2,200  of  6,362.\n",
            "  Batch 2,250  of  6,362.\n",
            "  Batch 2,300  of  6,362.\n",
            "  Batch 2,350  of  6,362.\n",
            "  Batch 2,400  of  6,362.\n",
            "  Batch 2,450  of  6,362.\n",
            "  Batch 2,500  of  6,362.\n",
            "  Batch 2,550  of  6,362.\n",
            "  Batch 2,600  of  6,362.\n",
            "  Batch 2,650  of  6,362.\n",
            "  Batch 2,700  of  6,362.\n",
            "  Batch 2,750  of  6,362.\n",
            "  Batch 2,800  of  6,362.\n",
            "  Batch 2,850  of  6,362.\n",
            "  Batch 2,900  of  6,362.\n",
            "  Batch 2,950  of  6,362.\n",
            "  Batch 3,000  of  6,362.\n",
            "  Batch 3,050  of  6,362.\n",
            "  Batch 3,100  of  6,362.\n",
            "  Batch 3,150  of  6,362.\n",
            "  Batch 3,200  of  6,362.\n",
            "  Batch 3,250  of  6,362.\n",
            "  Batch 3,300  of  6,362.\n",
            "  Batch 3,350  of  6,362.\n",
            "  Batch 3,400  of  6,362.\n",
            "  Batch 3,450  of  6,362.\n",
            "  Batch 3,500  of  6,362.\n",
            "  Batch 3,550  of  6,362.\n",
            "  Batch 3,600  of  6,362.\n",
            "  Batch 3,650  of  6,362.\n",
            "  Batch 3,700  of  6,362.\n",
            "  Batch 3,750  of  6,362.\n",
            "  Batch 3,800  of  6,362.\n",
            "  Batch 3,850  of  6,362.\n",
            "  Batch 3,900  of  6,362.\n",
            "  Batch 3,950  of  6,362.\n",
            "  Batch 4,000  of  6,362.\n",
            "  Batch 4,050  of  6,362.\n",
            "  Batch 4,100  of  6,362.\n",
            "  Batch 4,150  of  6,362.\n",
            "  Batch 4,200  of  6,362.\n",
            "  Batch 4,250  of  6,362.\n",
            "  Batch 4,300  of  6,362.\n",
            "  Batch 4,350  of  6,362.\n",
            "  Batch 4,400  of  6,362.\n",
            "  Batch 4,450  of  6,362.\n",
            "  Batch 4,500  of  6,362.\n",
            "  Batch 4,550  of  6,362.\n",
            "  Batch 4,600  of  6,362.\n",
            "  Batch 4,650  of  6,362.\n",
            "  Batch 4,700  of  6,362.\n",
            "  Batch 4,750  of  6,362.\n",
            "  Batch 4,800  of  6,362.\n",
            "  Batch 4,850  of  6,362.\n",
            "  Batch 4,900  of  6,362.\n",
            "  Batch 4,950  of  6,362.\n",
            "  Batch 5,000  of  6,362.\n",
            "  Batch 5,050  of  6,362.\n",
            "  Batch 5,100  of  6,362.\n",
            "  Batch 5,150  of  6,362.\n",
            "  Batch 5,200  of  6,362.\n",
            "  Batch 5,250  of  6,362.\n",
            "  Batch 5,300  of  6,362.\n",
            "  Batch 5,350  of  6,362.\n",
            "  Batch 5,400  of  6,362.\n",
            "  Batch 5,450  of  6,362.\n",
            "  Batch 5,500  of  6,362.\n",
            "  Batch 5,550  of  6,362.\n",
            "  Batch 5,600  of  6,362.\n",
            "  Batch 5,650  of  6,362.\n",
            "  Batch 5,700  of  6,362.\n",
            "  Batch 5,750  of  6,362.\n",
            "  Batch 5,800  of  6,362.\n",
            "  Batch 5,850  of  6,362.\n",
            "  Batch 5,900  of  6,362.\n",
            "  Batch 5,950  of  6,362.\n",
            "  Batch 6,000  of  6,362.\n",
            "  Batch 6,050  of  6,362.\n",
            "  Batch 6,100  of  6,362.\n",
            "  Batch 6,150  of  6,362.\n",
            "  Batch 6,200  of  6,362.\n",
            "  Batch 6,250  of  6,362.\n",
            "  Batch 6,300  of  6,362.\n",
            "  Batch 6,350  of  6,362.\n",
            "  Batch 6,362  of  6,362.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of  1,591.\n",
            "  Batch   100  of  1,591.\n",
            "  Batch   150  of  1,591.\n",
            "  Batch   200  of  1,591.\n",
            "  Batch   250  of  1,591.\n",
            "  Batch   300  of  1,591.\n",
            "  Batch   350  of  1,591.\n",
            "  Batch   400  of  1,591.\n",
            "  Batch   450  of  1,591.\n",
            "  Batch   500  of  1,591.\n",
            "  Batch   550  of  1,591.\n",
            "  Batch   600  of  1,591.\n",
            "  Batch   650  of  1,591.\n",
            "  Batch   700  of  1,591.\n",
            "  Batch   750  of  1,591.\n",
            "  Batch   800  of  1,591.\n",
            "  Batch   850  of  1,591.\n",
            "  Batch   900  of  1,591.\n",
            "  Batch   950  of  1,591.\n",
            "  Batch 1,000  of  1,591.\n",
            "  Batch 1,050  of  1,591.\n",
            "  Batch 1,100  of  1,591.\n",
            "  Batch 1,150  of  1,591.\n",
            "  Batch 1,200  of  1,591.\n",
            "  Batch 1,250  of  1,591.\n",
            "  Batch 1,300  of  1,591.\n",
            "  Batch 1,350  of  1,591.\n",
            "  Batch 1,400  of  1,591.\n",
            "  Batch 1,450  of  1,591.\n",
            "  Batch 1,500  of  1,591.\n",
            "  Batch 1,550  of  1,591.\n",
            "  Batch 1,591  of  1,591.\n",
            "\n",
            "Training Loss: 0.464\n",
            "Validation Loss: 0.446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Results"
      ],
      "metadata": {
        "id": "iwHz-9rZrRpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test safety classifier in batches to avoid CUDA out of memory error\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)  # Use the same batch size as training\n",
        "\n",
        "#load weights of best model\n",
        "path = args.save_path\n",
        "# path = 'new_distillbert_saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        sent_id, mask, labels = [t.to(device) for t in batch]\n",
        "        preds = model(sent_id, mask)[0]\n",
        "        preds = preds.detach().cpu().numpy()\n",
        "        all_preds.append(preds)\n",
        "\n",
        "all_preds = np.concatenate(all_preds, axis=0)  # Combine predictions from all batches\n",
        "preds = np.argmax(all_preds, axis=1)\n",
        "print(f'Testing Accuracy = {100*torch.sum(torch.tensor(preds) == test_y)/test_y.shape[0]}%')\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRF4Ax3rrKeR",
        "outputId": "c08bdfce-70db-4516-e5b3-36fac5910506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy = 73.4814224243164%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.43      0.34      7916\n",
            "           1       0.88      0.79      0.83     41423\n",
            "\n",
            "    accuracy                           0.73     49339\n",
            "   macro avg       0.58      0.61      0.59     49339\n",
            "weighted avg       0.78      0.73      0.76     49339\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWI-cjlPTk7e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}